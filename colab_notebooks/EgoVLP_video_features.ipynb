{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "J5JY6CpEaP_b",
        "outputId": "5760fb2d-019f-40a2-8cf8-6f779de23cc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Collecting decord\n",
            "  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from decord) (2.0.2)\n",
            "Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m123.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: decord\n",
            "Successfully installed decord-0.6.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy) (0.2.14)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy\n",
            "Successfully installed ftfy-6.3.1\n",
            "Collecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.12/dist-packages (from timm==0.4.12) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm==0.4.12) (0.24.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.20.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.12) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==0.4.12) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==0.4.12) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.4->timm==0.4.12) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.4->timm==0.4.12) (3.0.3)\n",
            "Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: timm\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.24\n",
            "    Uninstalling timm-1.0.24:\n",
            "      Successfully uninstalled timm-1.0.24\n",
            "Successfully installed timm-0.4.12\n",
            "Collecting av\n",
            "  Downloading av-16.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Downloading av-16.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (41.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-16.1.0\n",
            "Collecting ffmpeg-python\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from ffmpeg-python) (1.0.0)\n",
            "Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.2.0\n"
          ]
        }
      ],
      "source": [
        "# Setup: Mount Google Drive and install required dependencies\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install required packages for EgoVLP\n",
        "!pip install decord\n",
        "!pip install transformers ftfy regex tqdm\n",
        "!pip install timm==0.4.12\n",
        "!pip install av\n",
        "!pip install ffmpeg-python\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/AML_Project/3_EgoVLP')\n",
        "\n",
        "# Clone EgoVLP repository if not already present\n",
        "if not os.path.exists('EgoVLP-main'):\n",
        "    !git clone https://github.com/showlab/EgoVLP.git EgoVLP-main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 3: Extending Baselines to New Feature Extraction Backbone - EgoVLP\n",
        "\n",
        "## Overview\n",
        "\n",
        "This section extends the CaptainCook4D baselines by integrating a new feature extraction backbone: **EgoVLP** (Egocentric Vision-Language Pre-training).\n",
        "\n",
        "**Objectives:**\n",
        "- Extract video features using the EgoVLP backbone\n",
        "- Adapt the CaptainCook4D feature extraction pipeline for EgoVLP\n",
        "- Generate 256-dimensional feature vectors at 1-second intervals\n",
        "- Enable comparison of LSTM baseline with different feature modalities\n",
        "\n",
        "**EgoVLP Architecture:**\n",
        "- **Video Encoder**: SpaceTimeTransformer (16 frames sampled per second)\n",
        "- **Text Encoder**: BERT-base-uncased\n",
        "- **Feature Dimension**: 256\n",
        "- **Training**: Pre-trained on egocentric video-text pairs\n",
        "\n",
        "**Key Tasks:**\n",
        "1. Download and setup EgoVLP model and dependencies\n",
        "2. Extract video features per second from cooking videos\n",
        "3. Save features as compressed numpy arrays (.npz)\n",
        "4. Enable reproducibility and efficient batch processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409,
          "referenced_widgets": [
            "c8f03be4fac6499f81d12c0428da867b",
            "a9de632ca6f34fcfa25223c5bce05f5d",
            "11b48ad8713743c38f5a9065e80c04c1",
            "6eb8aa825295441aae803f362621c614",
            "2843b16408144187a16564cb0782b9c2",
            "0fcdbaccdfe245e88075a92bb4016813",
            "ef6588151fa54fe49ee6705f4e49dae7",
            "5b9f6ee0c733413ba440054f5729fc1b",
            "dea737b89b974ed89558614690f09643",
            "a543ef167d994091b5a30ce573ad0e4a",
            "e455de49065c479d9956123a7c2d15b8",
            "c5e45f644d5c4eca8893fc4c3daf5bef",
            "3f4c3682b6fc4dc3a3844eed5833082f",
            "1caccc5c629b4727bc50eb69203ee865",
            "f75ff9cd770b444f9591ed9bafabb725",
            "46117b76f40b48e28c117018832742ac",
            "2e14faabfa8440e487f07c02a0b6e8f0",
            "5399c3887c9442afa28ccd42e100a6a9",
            "4614ec70d5954d59baa7e63ac15901a9",
            "6b2b6bf7f0694aa89c2fa0f23190c9d2",
            "27b5f680ea6441d29a4feba3fd7581dd",
            "3b22e3e9c0f646ed810c0adf9ed172db"
          ]
        },
        "id": "LEXg6BiSam9z",
        "outputId": "d5bf4b11-9624-4489-fbd5-6b20e1cbbc8c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Backbone ImageNet trovato: pretrained/jx_vit_base_p16_224-80ecf9dd.pth\n",
            "Inizializzazione modello su cuda...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c8f03be4fac6499f81d12c0428da867b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5e45f644d5c4eca8893fc4c3daf5bef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "######USING ATTENTION STYLE:  frozen-in-time\n",
            "‚úÖ Trovati 384 video da elaborare.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Estrazione Video: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 384/384 [17:03:44<00:00, 159.96s/it, üîÑ Processing: 9_8_360p_224.mp4]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Completato. 223 nuovi video elaborati.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Part 3: EgoVLP Feature Extraction Pipeline for CaptainCook4D\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import glob\n",
        "import shutil\n",
        "import urllib.request\n",
        "from tqdm import tqdm\n",
        "from decord import VideoReader, cpu\n",
        "from torchvision.transforms import Compose, Lambda\n",
        "from torchvision.transforms._transforms_video import CenterCropVideo, NormalizeVideo\n",
        "\n",
        "# Path configuration\n",
        "DRIVE_ROOT = '/content/drive/MyDrive/AML_Project/3_EgoVLP'\n",
        "REPO_PATH = os.path.join(DRIVE_ROOT, 'EgoVLP-main')\n",
        "CHECKPOINT_PATH = os.path.join(DRIVE_ROOT, 'checkpoints/egovlp.pth')\n",
        "VIDEO_DIR = os.path.join(DRIVE_ROOT, 'videos')\n",
        "FEATURES_DIR = os.path.join(DRIVE_ROOT, 'features')\n",
        "TEMP_WORK_DIR = '/content/temp_video_processing'\n",
        "\n",
        "# --- Setup and Utilities ---\n",
        "def ensure_pretrained_backbone():\n",
        "    \"\"\"\n",
        "    Downloads ViT backbone weights (ImageNet) if not present.\n",
        "    \n",
        "    Required because EgoVLP/TimeSformer looks for this file statically in ./pretrained/\n",
        "    This function handles automatic download with progress tracking.\n",
        "    \n",
        "    Raises:\n",
        "        RuntimeError: If backbone download fails or connection is unavailable\n",
        "    \"\"\"\n",
        "    backbone_dir = \"pretrained\"\n",
        "    filename = \"jx_vit_base_p16_224-80ecf9dd.pth\"\n",
        "    file_path = os.path.join(backbone_dir, filename)\n",
        "    url = \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_224-80ecf9dd.pth\"\n",
        "\n",
        "    if not os.path.exists(backbone_dir):\n",
        "        os.makedirs(backbone_dir)\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"Backbone '{filename}' missing. Download in progress...\")\n",
        "        try:\n",
        "            # Use urllib with simple text progress bar\n",
        "            with tqdm(unit='B', unit_scale=True, miniters=1, desc=filename) as t:\n",
        "                def reporthook(blocknum, blocksize, totalsize):\n",
        "                    t.total = totalsize\n",
        "                    t.update(blocknum * blocksize - t.n)\n",
        "                urllib.request.urlretrieve(url, file_path, reporthook=reporthook)\n",
        "            print(\"‚úÖ Backbone download completed.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Backbone download error: {e}\")\n",
        "            raise RuntimeError(\"Unable to download base ImageNet weights. Check your connection.\")\n",
        "    else:\n",
        "        print(f\"‚úÖ ImageNet backbone found: {file_path}\")\n",
        "\n",
        "def setup_environment():\n",
        "    \"\"\"\n",
        "    Initializes the EgoVLP environment.\n",
        "    \n",
        "    Sets up:\n",
        "    - Python path to include EgoVLP repository\n",
        "    - Temporary directory for video processing\n",
        "    - GPU optimization flags\n",
        "    - Ensures backbone weights are downloaded\n",
        "    \"\"\"\n",
        "    if REPO_PATH not in sys.path:\n",
        "        sys.path.append(REPO_PATH)\n",
        "    if not os.path.exists(TEMP_WORK_DIR):\n",
        "        os.makedirs(TEMP_WORK_DIR)\n",
        "\n",
        "    # GPU optimization\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # Ensure base weights are present BEFORE importing or loading the model\n",
        "    ensure_pretrained_backbone()\n",
        "\n",
        "setup_environment()\n",
        "\n",
        "try:\n",
        "    from model.model import FrozenInTime\n",
        "except ImportError as e:\n",
        "    # Dynamic handling of missing dependencies\n",
        "    missing_module = e.name\n",
        "    print(f\"Critical import error: {e}\")\n",
        "\n",
        "    suggestion = \"\"\n",
        "    if missing_module == 'av':\n",
        "        suggestion = \"Run: !pip install av\"\n",
        "    elif missing_module == 'ffmpeg':\n",
        "        suggestion = \"Run: !pip install ffmpeg-python\"\n",
        "    elif missing_module == 'cv2':\n",
        "        suggestion = \"Run: !pip install opencv-python\"\n",
        "    else:\n",
        "        suggestion = f\"Try running: !pip install {missing_module}\"\n",
        "\n",
        "    raise RuntimeError(f\"Missing critical dependency. {suggestion}\")\n",
        "\n",
        "# --- Model Loading ---\n",
        "def get_egovlp_model(checkpoint_path, device='cuda'):\n",
        "    \"\"\"\n",
        "    Initializes and loads the EgoVLP model.\n",
        "    \n",
        "    Creates a FrozenInTime model with:\n",
        "    - SpaceTimeTransformer video encoder (16 frames per second)\n",
        "    - BERT text encoder\n",
        "    - 256-dimensional projection layer\n",
        "    \n",
        "    Args:\n",
        "        checkpoint_path: Path to the pre-trained EgoVLP checkpoint\n",
        "        device: Device to load model on ('cuda' or 'cpu')\n",
        "    \n",
        "    Returns:\n",
        "        model: Loaded EgoVLP model in evaluation mode\n",
        "    \n",
        "    Raises:\n",
        "        FileNotFoundError: If checkpoint path does not exist\n",
        "    \"\"\"\n",
        "    print(f\"Initializing model on {device}...\")\n",
        "    model = FrozenInTime(\n",
        "        video_params={\"model\": \"SpaceTimeTransformer\", \"pretrained\": True, \"num_frames\": 16},\n",
        "        text_params={\"model\": \"bert-base-uncased\", \"pretrained\": True},\n",
        "        projection_dim=256,\n",
        "        load_checkpoint=None\n",
        "    )\n",
        "\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)\n",
        "        state_dict = checkpoint['state_dict'] if 'state_dict' in checkpoint else checkpoint\n",
        "        new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
        "        model.load_state_dict(new_state_dict, strict=False)\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# --- Preprocessing ---\n",
        "def get_transform(input_size=224):\n",
        "    \"\"\"\n",
        "    Creates video preprocessing pipeline.\n",
        "    \n",
        "    Applies standard normalization for ImageNet pre-trained models:\n",
        "    - Rescale pixel values to [0, 1]\n",
        "    - Normalize using ImageNet statistics\n",
        "    - Center crop to (224, 224)\n",
        "    \n",
        "    Args:\n",
        "        input_size: Target spatial dimension (default: 224)\n",
        "    \n",
        "    Returns:\n",
        "        Compose: PyTorch transforms pipeline\n",
        "    \"\"\"\n",
        "    mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "    return Compose([\n",
        "        Lambda(lambda x: x / 255.0),\n",
        "        NormalizeVideo(mean, std),\n",
        "        CenterCropVideo(input_size),\n",
        "    ])\n",
        "\n",
        "# --- Feature Extraction ---\n",
        "def extract_features_per_second(model, video_path, device, batch_size_inference=16):\n",
        "    \"\"\"\n",
        "    Extracts EgoVLP features second-by-second from a video.\n",
        "    \n",
        "    For each second of the video:\n",
        "    1. Load video and compute FPS\n",
        "    2. Sample 16 frames evenly spaced within the second\n",
        "    3. Apply preprocessing (normalization, center crop)\n",
        "    4. Pass through EgoVLP model to get 256-dim features\n",
        "    5. Batch process for efficiency\n",
        "    \n",
        "    Args:\n",
        "        model: Pre-loaded EgoVLP model\n",
        "        video_path: Path to video file\n",
        "        device: Device to process on ('cuda' or 'cpu')\n",
        "        batch_size_inference: Number of seconds to process in parallel (default: 16)\n",
        "    \n",
        "    Returns:\n",
        "        np.ndarray: Features array of shape (num_seconds, 256)\n",
        "                   Returns None if video is too short (< 1 second)\n",
        "    \"\"\"\n",
        "    # Load video\n",
        "    vr = VideoReader(video_path, ctx=cpu(0))\n",
        "    fps = vr.get_avg_fps()\n",
        "    total_frames = len(vr)\n",
        "    duration_sec = int(total_frames / fps)\n",
        "\n",
        "    if duration_sec == 0:\n",
        "        return None  # Video too short (< 1 second)\n",
        "\n",
        "    transform = get_transform()\n",
        "\n",
        "    features_list = []\n",
        "    batch_buffer = []\n",
        "\n",
        "    # print(f\"  -> Video duration: {duration_sec}s. FPS: {fps:.2f}. Batch: {batch_size_inference}\")\n",
        "\n",
        "    for sec in range(duration_sec):\n",
        "        # Define the temporal interval of the current second\n",
        "        start_frame = int(sec * fps)\n",
        "        end_frame = int((sec + 1) * fps)\n",
        "\n",
        "        # Avoid index out of bounds\n",
        "        end_frame = min(end_frame, total_frames)\n",
        "\n",
        "        # Sample 16 frames evenly spaced in this second\n",
        "        if end_frame - start_frame < 16:\n",
        "             frame_indices = np.linspace(start_frame, end_frame - 1, 16, dtype=int)\n",
        "        else:\n",
        "             frame_indices = np.linspace(start_frame, end_frame - 1, 16, dtype=int)\n",
        "\n",
        "        raw_frames = vr.get_batch(frame_indices)  # (T, H, W, C)\n",
        "\n",
        "        if isinstance(raw_frames, torch.Tensor):\n",
        "            frames_tensor = raw_frames\n",
        "        else:\n",
        "            frames_tensor = torch.tensor(raw_frames.asnumpy())\n",
        "\n",
        "        # Standard PyTorch Video: (T, H, W, C) -> (C, T, H, W)\n",
        "        frames_tensor = frames_tensor.permute(3, 0, 1, 2).float()\n",
        "\n",
        "        # Normalization: input (C, T, H, W) -> output (C, T, H, W)\n",
        "        transformed_frames = transform(frames_tensor)\n",
        "\n",
        "        # EgoVLP requires: (Time, Channels, Height, Width) for single element\n",
        "        # Permute from (C, T, H, W) to (T, C, H, W)\n",
        "        transformed_frames = transformed_frames.permute(1, 0, 2, 3)\n",
        "\n",
        "        batch_buffer.append(transformed_frames)\n",
        "\n",
        "        # If buffer is full, perform inference\n",
        "        if len(batch_buffer) == batch_size_inference:\n",
        "            input_tensor = torch.stack(batch_buffer).to(device)\n",
        "            with torch.no_grad():\n",
        "                feat_batch = model({'video': input_tensor}, video_only=True)\n",
        "                features_list.append(feat_batch.cpu().numpy())\n",
        "            batch_buffer = []\n",
        "\n",
        "    # Process any remaining items in buffer\n",
        "    if len(batch_buffer) > 0:\n",
        "        input_tensor = torch.stack(batch_buffer).to(device)\n",
        "        with torch.no_grad():\n",
        "            feat_batch = model({'video': input_tensor}, video_only=True)\n",
        "            features_list.append(feat_batch.cpu().numpy())\n",
        "\n",
        "    # Concatenate all batches: output (Total_Seconds, 256)\n",
        "    if features_list:\n",
        "        return np.concatenate(features_list, axis=0)\n",
        "    return np.array([])\n",
        "\n",
        "def process_all_videos():\n",
        "    \"\"\"\n",
        "    Main processing function: extracts EgoVLP features for all videos.\n",
        "    \n",
        "    Workflow:\n",
        "    1. Initialize EgoVLP model on GPU/CPU\n",
        "    2. Discover all video files in VIDEO_DIR\n",
        "    3. For each video:\n",
        "       - Skip if features already exist\n",
        "       - Copy video to temporary local storage\n",
        "       - Extract features second-by-second\n",
        "       - Save features as compressed .npz file\n",
        "       - Clean up temporary files\n",
        "    4. Report success count\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    try:\n",
        "        model = get_egovlp_model(CHECKPOINT_PATH, device)\n",
        "    except Exception as e:\n",
        "        print(f\"Critical model error: {e}\")\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(FEATURES_DIR):\n",
        "        os.makedirs(FEATURES_DIR)\n",
        "\n",
        "    video_files = []\n",
        "    for ext in ['*.mp4', '*.MP4', '*.avi', '*.mov']:\n",
        "        video_files.extend(glob.glob(os.path.join(VIDEO_DIR, ext)))\n",
        "    video_files = sorted(list(set(video_files)))\n",
        "\n",
        "    print(f\"Found {len(video_files)} videos to process.\")\n",
        "\n",
        "    count_success = 0\n",
        "\n",
        "    # Progress bar with dynamic refresh\n",
        "    pbar = tqdm(video_files, desc=\"Feature Extraction\")\n",
        "\n",
        "    for drive_video_path in pbar:\n",
        "        video_name = os.path.basename(drive_video_path)\n",
        "        feature_filename = f\"{video_name}_1s_1s.npz\"\n",
        "        feature_save_path = os.path.join(FEATURES_DIR, feature_filename)\n",
        "\n",
        "        # --- Skip logic ---\n",
        "        if os.path.exists(feature_save_path):\n",
        "            pbar.set_postfix_str(f\"‚è© Skip: {video_name}\")\n",
        "            continue\n",
        "\n",
        "        local_temp_path = os.path.join(TEMP_WORK_DIR, video_name)\n",
        "\n",
        "        try:\n",
        "            pbar.set_postfix_str(f\"üîÑ Processing: {video_name}\")\n",
        "            shutil.copy(drive_video_path, local_temp_path)\n",
        "\n",
        "            features = extract_features_per_second(model, local_temp_path, device)\n",
        "\n",
        "            if features is not None and len(features) > 0:\n",
        "                np.savez_compressed(feature_save_path, features=features)\n",
        "                count_success += 1\n",
        "            else:\n",
        "                print(f\"\\nNo features extracted for {video_name} (too short?)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError with {video_name}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "        finally:\n",
        "            if os.path.exists(local_temp_path):\n",
        "                os.remove(local_temp_path)\n",
        "\n",
        "    print(f\"\\n‚úÖ Completed. {count_success} new videos processed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_all_videos()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHIjw7L3cT2D"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0fcdbaccdfe245e88075a92bb4016813": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11b48ad8713743c38f5a9065e80c04c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b9f6ee0c733413ba440054f5729fc1b",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dea737b89b974ed89558614690f09643",
            "value": 570
          }
        },
        "1caccc5c629b4727bc50eb69203ee865": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4614ec70d5954d59baa7e63ac15901a9",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b2b6bf7f0694aa89c2fa0f23190c9d2",
            "value": 440449768
          }
        },
        "27b5f680ea6441d29a4feba3fd7581dd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2843b16408144187a16564cb0782b9c2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e14faabfa8440e487f07c02a0b6e8f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b22e3e9c0f646ed810c0adf9ed172db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f4c3682b6fc4dc3a3844eed5833082f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e14faabfa8440e487f07c02a0b6e8f0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5399c3887c9442afa28ccd42e100a6a9",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "46117b76f40b48e28c117018832742ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4614ec70d5954d59baa7e63ac15901a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5399c3887c9442afa28ccd42e100a6a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b9f6ee0c733413ba440054f5729fc1b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b2b6bf7f0694aa89c2fa0f23190c9d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6eb8aa825295441aae803f362621c614": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a543ef167d994091b5a30ce573ad0e4a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e455de49065c479d9956123a7c2d15b8",
            "value": "‚Äá570/570‚Äá[00:00&lt;00:00,‚Äá63.8kB/s]"
          }
        },
        "a543ef167d994091b5a30ce573ad0e4a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9de632ca6f34fcfa25223c5bce05f5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fcdbaccdfe245e88075a92bb4016813",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ef6588151fa54fe49ee6705f4e49dae7",
            "value": "config.json:‚Äá100%"
          }
        },
        "c5e45f644d5c4eca8893fc4c3daf5bef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f4c3682b6fc4dc3a3844eed5833082f",
              "IPY_MODEL_1caccc5c629b4727bc50eb69203ee865",
              "IPY_MODEL_f75ff9cd770b444f9591ed9bafabb725"
            ],
            "layout": "IPY_MODEL_46117b76f40b48e28c117018832742ac"
          }
        },
        "c8f03be4fac6499f81d12c0428da867b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a9de632ca6f34fcfa25223c5bce05f5d",
              "IPY_MODEL_11b48ad8713743c38f5a9065e80c04c1",
              "IPY_MODEL_6eb8aa825295441aae803f362621c614"
            ],
            "layout": "IPY_MODEL_2843b16408144187a16564cb0782b9c2"
          }
        },
        "dea737b89b974ed89558614690f09643": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e455de49065c479d9956123a7c2d15b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef6588151fa54fe49ee6705f4e49dae7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f75ff9cd770b444f9591ed9bafabb725": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27b5f680ea6441d29a4feba3fd7581dd",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3b22e3e9c0f646ed810c0adf9ed172db",
            "value": "‚Äá440M/440M‚Äá[00:03&lt;00:00,‚Äá247MB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
