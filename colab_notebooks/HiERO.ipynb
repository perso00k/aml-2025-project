{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Extension Substep 1: Recipe Step Localization using HiERO\n",
        "\n",
        "## Objective\n",
        "Segment individual steps of recipe videos using a zero-shot clustering-based approach (HiERO). \n",
        "\n",
        "## Workflow\n",
        "1. **Load pre-trained HiERO model**: Uses a graph neural network trained on hierarchical temporal modeling\n",
        "2. **Extract hierarchical features**: Computes multi-scale temporal representations from video features\n",
        "3. **Cluster and segment**: Uses spectral clustering with median filtering to identify temporal boundaries\n",
        "4. **Compute step embeddings**: Averages video features within detected step boundaries to obtain step-level representations\n",
        "\n",
        "## Output\n",
        "For each video: \n",
        "- A list of tuples `(start_time, end_time)` defining step boundaries\n",
        "- A sequence of step-level embeddings (averaged features for each detected step)\n",
        "- Cluster labels indicating temporal boundaries\n",
        "\n",
        "## Key Technologies\n",
        "- **HiERO**: Hierarchical temporal segmentation using graph neural networks\n",
        "- **Spectral Clustering**: Zero-shot clustering for temporal boundary detection\n",
        "- **Median Filtering**: Temporal smoothing to reduce noise in segmentation labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "P_-Fs_Gn1XJ0",
        "outputId": "597263c9-65da-4391-f592-287af9be3afa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Working directory set to: /content/drive/MyDrive/AML_Project/Extension/step1_HiERO/HiERO\n",
            "/content/drive/.shortcut-targets-by-id/1vxgD6uYnr2LpQalDA9eOLzCE7O5v_LHt/AML_Project/Extension/step1_HiERO/HiERO\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.4.0+cu124.html\n",
            "Requirement already satisfied: absl-py==2.3.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs==2.6.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (2.6.1)\n",
            "Requirement already satisfied: aiohttp==3.12.15 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (3.12.15)\n",
            "Requirement already satisfied: aiosignal==1.4.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (1.4.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (4.9.3)\n",
            "Requirement already satisfied: asttokens==3.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (3.0.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (1.6.3)\n",
            "Requirement already satisfied: attrs==25.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (25.3.0)\n",
            "Requirement already satisfied: certifi==2025.8.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (2025.8.3)\n",
            "Requirement already satisfied: charset-normalizer==3.4.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (3.4.3)\n",
            "Requirement already satisfied: colorlog==6.9.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (6.9.0)\n",
            "Requirement already satisfied: comm==0.2.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (0.2.3)\n",
            "Requirement already satisfied: debugpy==1.8.16 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 13)) (1.8.16)\n",
            "Requirement already satisfied: decorator==5.2.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 14)) (5.2.1)\n",
            "Requirement already satisfied: dm-tree==0.1.9 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 15)) (0.1.9)\n",
            "Requirement already satisfied: einops==0.8.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 16)) (0.8.1)\n",
            "Requirement already satisfied: executing==2.2.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 17)) (2.2.0)\n",
            "Requirement already satisfied: filelock==3.19.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 18)) (3.19.1)\n",
            "Requirement already satisfied: frozenlist==1.7.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 19)) (1.7.0)\n",
            "Requirement already satisfied: fsspec==2025.7.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 20)) (2025.7.0)\n",
            "Requirement already satisfied: ftfy==6.3.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 21)) (6.3.1)\n",
            "Requirement already satisfied: gast==0.6.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 22)) (0.6.0)\n",
            "Requirement already satisfied: hf-xet==1.1.8 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 23)) (1.1.8)\n",
            "Requirement already satisfied: huggingface-hub==0.34.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 24)) (0.34.4)\n",
            "Requirement already satisfied: hydra-colorlog==1.2.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 25)) (1.2.0)\n",
            "Requirement already satisfied: hydra-core==1.3.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 26)) (1.3.2)\n",
            "Requirement already satisfied: idna==3.10 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 27)) (3.10)\n",
            "Requirement already satisfied: iopath==0.1.10 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 28)) (0.1.10)\n",
            "Requirement already satisfied: jedi==0.19.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 29)) (0.19.2)\n",
            "Requirement already satisfied: Jinja2==3.1.6 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 30)) (3.1.6)\n",
            "Requirement already satisfied: joblib==1.5.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 31)) (1.5.1)\n",
            "Requirement already satisfied: jupyter_client==8.6.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 32)) (8.6.3)\n",
            "Requirement already satisfied: jupyter_core==5.8.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 33)) (5.8.1)\n",
            "Requirement already satisfied: lightning-utilities==0.15.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 34)) (0.15.2)\n",
            "Requirement already satisfied: MarkupSafe==3.0.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 35)) (3.0.2)\n",
            "Requirement already satisfied: matplotlib-inline==0.1.7 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 36)) (0.1.7)\n",
            "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 37)) (1.3.0)\n",
            "Requirement already satisfied: multidict==6.6.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 38)) (6.6.4)\n",
            "Requirement already satisfied: nest-asyncio==1.6.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 39)) (1.6.0)\n",
            "Requirement already satisfied: networkx==3.5 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 40)) (3.5)\n",
            "Requirement already satisfied: numpy==2.2.6 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 41)) (2.2.6)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.2.65 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 42)) (12.4.2.65)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.99 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 43)) (12.4.99)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.99 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 44)) (12.4.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.99 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 45)) (12.4.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 46)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.0.44 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 47)) (11.2.0.44)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 48)) (1.13.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.119 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 49)) (10.3.5.119)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.0.99 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 50)) (11.6.0.99)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.0.142 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 51)) (12.3.0.142)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 52)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-dali-cuda120==1.51.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 53)) (1.51.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 54)) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvcomp-cu12==5.0.0.6 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 55)) (5.0.0.6)\n",
            "Requirement already satisfied: nvidia-nvimgcodec-cu12==0.6.0.32 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 56)) (0.6.0.32)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.99 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 57)) (12.4.99)\n",
            "Requirement already satisfied: nvidia-nvjpeg-cu12==12.4.0.76 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 58)) (12.4.0.76)\n",
            "Requirement already satisfied: nvidia-nvjpeg2k-cu12==0.9.0.43 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 59)) (0.9.0.43)\n",
            "Requirement already satisfied: nvidia-nvtiff-cu12==0.5.1.75 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 60)) (0.5.1.75)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.99 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 61)) (12.4.99)\n",
            "Requirement already satisfied: omegaconf==2.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 62)) (2.3.0)\n",
            "Requirement already satisfied: opencv-python==4.12.0.88 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 63)) (4.12.0.88)\n",
            "Requirement already satisfied: packaging==25.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 64)) (25.0)\n",
            "Requirement already satisfied: pandas==2.3.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 65)) (2.3.2)\n",
            "Requirement already satisfied: parso==0.8.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 66)) (0.8.4)\n",
            "Requirement already satisfied: pexpect==4.9.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 67)) (4.9.0)\n",
            "Requirement already satisfied: pillow==11.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 68)) (11.3.0)\n",
            "Requirement already satisfied: platformdirs==4.3.8 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 69)) (4.3.8)\n",
            "Requirement already satisfied: portalocker==3.2.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 70)) (3.2.0)\n",
            "Requirement already satisfied: prompt_toolkit==3.0.51 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 71)) (3.0.51)\n",
            "Requirement already satisfied: propcache==0.3.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 72)) (0.3.2)\n",
            "Requirement already satisfied: psutil==7.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 73)) (7.0.0)\n",
            "Requirement already satisfied: ptyprocess==0.7.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 74)) (0.7.0)\n",
            "Requirement already satisfied: pure_eval==0.2.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 75)) (0.2.3)\n",
            "Requirement already satisfied: pyg-lib==0.4.0+pt24cu124 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 76)) (0.4.0+pt24cu124)\n",
            "Requirement already satisfied: Pygments==2.19.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 77)) (2.19.2)\n",
            "Requirement already satisfied: pyparsing==3.2.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 78)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 79)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz==2025.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 80)) (2025.2)\n",
            "Requirement already satisfied: PyYAML==6.0.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 81)) (6.0.2)\n",
            "Requirement already satisfied: pyzmq==27.0.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 82)) (27.0.2)\n",
            "Requirement already satisfied: regex==2025.7.34 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 83)) (2025.7.34)\n",
            "Requirement already satisfied: requests==2.32.5 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 84)) (2.32.5)\n",
            "Requirement already satisfied: safetensors==0.6.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 85)) (0.6.2)\n",
            "Requirement already satisfied: scikit-learn==1.7.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 86)) (1.7.1)\n",
            "Requirement already satisfied: scipy==1.16.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 87)) (1.16.1)\n",
            "Requirement already satisfied: six==1.17.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 88)) (1.17.0)\n",
            "Requirement already satisfied: stack-data==0.6.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 89)) (0.6.3)\n",
            "Requirement already satisfied: sympy==1.14.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 90)) (1.14.0)\n",
            "Requirement already satisfied: terminaltables==3.1.10 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 91)) (3.1.10)\n",
            "Requirement already satisfied: threadpoolctl==3.6.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 92)) (3.6.0)\n",
            "Requirement already satisfied: timm==1.0.19 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 93)) (1.0.19)\n",
            "Requirement already satisfied: tokenizers==0.21.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 94)) (0.21.4)\n",
            "Requirement already satisfied: torch==2.4.1+cu124 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 95)) (2.4.1+cu124)\n",
            "Requirement already satisfied: torch-geometric==2.6.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 96)) (2.6.1)\n",
            "Requirement already satisfied: torch-kmeans==0.2.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 97)) (0.2.0)\n",
            "Requirement already satisfied: torch_cluster==1.6.3+pt24cu124 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 98)) (1.6.3+pt24cu124)\n",
            "Requirement already satisfied: torch_scatter==2.1.2+pt24cu124 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 99)) (2.1.2+pt24cu124)\n",
            "Requirement already satisfied: torch_sparse==0.6.18+pt24cu124 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 100)) (0.6.18+pt24cu124)\n",
            "Requirement already satisfied: torch_spline_conv==1.2.2+pt24cu124 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 101)) (1.2.2+pt24cu124)\n",
            "Requirement already satisfied: torchaudio==2.4.1+cu124 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 102)) (2.4.1+cu124)\n",
            "Requirement already satisfied: torchmetrics==1.8.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 103)) (1.8.1)\n",
            "Requirement already satisfied: torchvision==0.19.1+cu124 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 104)) (0.19.1+cu124)\n",
            "Requirement already satisfied: tornado==6.5.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 105)) (6.5.2)\n",
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 106)) (4.67.1)\n",
            "Requirement already satisfied: traitlets==5.14.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 107)) (5.14.3)\n",
            "Requirement already satisfied: transformers==4.55.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 108)) (4.55.4)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 109)) (3.0.0)\n",
            "Requirement already satisfied: typing_extensions==4.14.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 110)) (4.14.1)\n",
            "Requirement already satisfied: tzdata==2025.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 111)) (2025.2)\n",
            "Requirement already satisfied: urllib3==2.5.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 112)) (2.5.0)\n",
            "Requirement already satisfied: wcwidth==0.2.13 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 113)) (0.2.13)\n",
            "Requirement already satisfied: wrapt==1.17.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 114)) (1.17.3)\n",
            "Requirement already satisfied: yarl==1.20.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 115)) (1.20.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse==1.6.3->-r requirements.txt (line 7)) (0.45.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities==0.15.2->-r requirements.txt (line 34)) (75.2.0)\n",
            "Requirement already satisfied: hydra-core in /usr/local/lib/python3.12/dist-packages (1.3.2)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.12/dist-packages (from hydra-core) (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core) (4.9.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from hydra-core) (25.0)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from omegaconf<2.4,>=2.2->hydra-core) (6.0.2)\n"
          ]
        }
      ],
      "source": [
        "# @title 1. Setup Environment & Path Configuration\n",
        "\"\"\"\n",
        "Initialize the environment by mounting Google Drive and installing required dependencies.\n",
        "This setup ensures all necessary libraries and project code are accessible.\n",
        "\"\"\"\n",
        "import os\n",
        "import sys\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access project files and data\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- PATH CONFIGURATION ---\n",
        "# Define the project root directory where files are stored on Drive\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive/AML_Project/Extension/step1_HiERO\"\n",
        "\n",
        "# Locate the HiERO code directory\n",
        "REPO_DIR = os.path.join(DRIVE_ROOT, \"HiERO\")\n",
        "\n",
        "# Verify the directory exists\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    raise FileNotFoundError(f\"Could not find code directory at: {REPO_DIR}. Please ensure the 'HiERO' folder has been correctly moved.\")\n",
        "\n",
        "print(f\"Working directory set to: {REPO_DIR}\")\n",
        "\n",
        "# Install Dependencies\n",
        "# Install required packages from requirements.txt with PyTorch GPU support\n",
        "%cd \"{REPO_DIR}\"\n",
        "!pip install -r requirements.txt -f https://data.pyg.org/whl/torch-2.4.0+cu124.html --extra-index-url https://download.pytorch.org/whl/\n",
        "!pip install hydra-core --upgrade\n",
        "\n",
        "# Add HiERO to Python path\n",
        "# Essential for Python to locate modules such as 'models', 'utils', etc.\n",
        "if REPO_DIR not in sys.path:\n",
        "    sys.path.append(REPO_DIR)\n",
        "\n",
        "print(\"Environment setup completed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byBM5e-f2sju"
      },
      "outputs": [],
      "source": [
        "# @title 2. Utility Functions (Model & Clustering)\n",
        "\"\"\"\n",
        "Core utility functions for HiERO model inference, hierarchical feature extraction,\n",
        "and temporal segmentation via spectral clustering.\n",
        "\"\"\"\n",
        "import torch\n",
        "import hydra\n",
        "import numpy as np\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from torch.nn import functional as F\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def build_hiero_model(ckpt_path, input_size=256, depth=2):\n",
        "    \"\"\"\n",
        "    Load the HiERO model from a checkpoint file.\n",
        "    \n",
        "    The model is instantiated with configuration saved in the checkpoint,\n",
        "    enabling hierarchical graph neural network processing.\n",
        "    \n",
        "    Args:\n",
        "        ckpt_path (str): Path to the model checkpoint file\n",
        "        input_size (int): Input feature dimension (default: 256 for EgoVLP)\n",
        "        depth (int): Hierarchical depth for feature extraction (default: 2)\n",
        "    \n",
        "    Returns:\n",
        "        torch.nn.Module: Loaded HiERO model in evaluation mode on the appropriate device\n",
        "    \n",
        "    Raises:\n",
        "        FileNotFoundError: If the checkpoint file does not exist\n",
        "    \"\"\"\n",
        "    if not os.path.exists(ckpt_path):\n",
        "        raise FileNotFoundError(f\"Checkpoint not found at: {ckpt_path}\")\n",
        "\n",
        "    print(f\"Loading model from {ckpt_path}...\")\n",
        "    weights = torch.load(ckpt_path, map_location=DEVICE)\n",
        "\n",
        "    # Instantiate model using saved configuration\n",
        "    model = hydra.utils.instantiate(\n",
        "        weights[\"config\"][\"model\"],\n",
        "        clustering_at_inference=True,\n",
        "        input_size=input_size,\n",
        "        _recursive_=False\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    model.load_state_dict(weights[\"model\"], strict=False)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def extract_hiero_features(model, features, depth=2):\n",
        "    \"\"\"\n",
        "    Perform inference to obtain hierarchical feature representations.\n",
        "    \n",
        "    Constructs a temporal graph from input features and extracts\n",
        "    multi-scale temporal representations at the specified hierarchical depth.\n",
        "    \n",
        "    Args:\n",
        "        model: HiERO model instance\n",
        "        features (torch.Tensor): Input video features of shape (T, feature_dim)\n",
        "        depth (int): Hierarchical level for feature extraction (default: 2)\n",
        "    \n",
        "    Returns:\n",
        "        torch.Tensor: Hierarchical features at the specified depth\n",
        "    \"\"\"\n",
        "    features = features.to(DEVICE)\n",
        "\n",
        "    # Build dummy temporal graph structure\n",
        "    pos = torch.arange(0, features.shape[0], device=DEVICE).float()\n",
        "    indices = torch.arange(0, features.shape[0], device=DEVICE)\n",
        "    batch = torch.zeros_like(indices, dtype=torch.long)\n",
        "    mask = torch.ones_like(indices, dtype=torch.bool)\n",
        "\n",
        "    data = Data(x=features.unsqueeze(1), pos=pos, indices=indices, batch=batch, mask=mask)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        graphs = model(data)\n",
        "        out_features = graphs.x[graphs.depth == depth]\n",
        "\n",
        "    return out_features\n",
        "\n",
        "def clusterize_segments(features, n_clusters=5, temp=0.05):\n",
        "    \"\"\"\n",
        "    Cluster features using spectral clustering to identify temporal segments.\n",
        "    \n",
        "    Computes similarity affinity matrix using cosine distance with temperature scaling,\n",
        "    then applies spectral clustering to group temporally similar features.\n",
        "    \n",
        "    Args:\n",
        "        features (torch.Tensor): Features to cluster of shape (N, feature_dim)\n",
        "        n_clusters (int): Target number of clusters (default: 5)\n",
        "        temp (float): Temperature parameter for affinity computation (default: 0.05)\n",
        "    \n",
        "    Returns:\n",
        "        np.ndarray: Cluster labels for each temporal position\n",
        "    \"\"\"\n",
        "    features = F.normalize(features, p=2, dim=-1)\n",
        "    affinity = torch.exp((features @ features.T) / temp).cpu().numpy()\n",
        "    sc = SpectralClustering(n_clusters=n_clusters, affinity=\"precomputed\", assign_labels='kmeans')\n",
        "    labels = sc.fit_predict(affinity)\n",
        "    return labels\n",
        "\n",
        "def get_segments_timestamps(labels, fps, stride=16, depth=2):\n",
        "    \"\"\"\n",
        "    Convert cluster labels to temporal segment boundaries.\n",
        "    \n",
        "    Identifies transitions in cluster labels and converts them to\n",
        "    (start_time, end_time) tuples based on video FPS and feature stride.\n",
        "    \n",
        "    Args:\n",
        "        labels (np.ndarray): Cluster labels for each position\n",
        "        fps (float): Frames per second of the original video\n",
        "        stride (int): Frame stride used in feature extraction (default: 16)\n",
        "        depth (int): Hierarchical depth (affects temporal scale) (default: 2)\n",
        "    \n",
        "    Returns:\n",
        "        list: List of (start_time, end_time) tuples in seconds\n",
        "    \"\"\"\n",
        "    seconds_per_block = (stride / fps) * (2**depth)\n",
        "    segments = []\n",
        "    if len(labels) == 0:\n",
        "        return segments\n",
        "\n",
        "    current_label = labels[0]\n",
        "    start_idx = 0\n",
        "\n",
        "    for i, label in enumerate(labels):\n",
        "        if label != current_label:\n",
        "            end_idx = i\n",
        "            start_time = start_idx * seconds_per_block\n",
        "            end_time = end_idx * seconds_per_block\n",
        "            segments.append((start_time, end_time))\n",
        "            current_label = label\n",
        "            start_idx = i\n",
        "\n",
        "    # Append final segment\n",
        "    start_time = start_idx * seconds_per_block\n",
        "    end_time = len(labels) * seconds_per_block\n",
        "    segments.append((start_time, end_time))\n",
        "\n",
        "    return segments\n",
        "\n",
        "def average_features_within_steps(original_features, segments, fps, stride=16):\n",
        "    \"\"\"\n",
        "    Compute step-level embeddings by averaging features within temporal boundaries.\n",
        "    \n",
        "    For each detected step (segment), pools the original video features that fall\n",
        "    within the segment boundaries, creating a single representative embedding.\n",
        "    \n",
        "    Args:\n",
        "        original_features (torch.Tensor): Original video features (T, feature_dim)\n",
        "        segments (list): List of (start_time, end_time) tuples\n",
        "        fps (float): Frames per second of the video\n",
        "        stride (int): Frame stride used in feature extraction (default: 16)\n",
        "    \n",
        "    Returns:\n",
        "        np.ndarray: Step-level embeddings, shape (num_segments, feature_dim)\n",
        "    \"\"\"\n",
        "    step_embeddings = []\n",
        "    feat_duration = stride / fps\n",
        "\n",
        "    for start, end in segments:\n",
        "        start_idx = int(start / feat_duration)\n",
        "        end_idx = int(end / feat_duration)\n",
        "        start_idx = max(0, start_idx)\n",
        "        end_idx = min(len(original_features), max(start_idx + 1, end_idx))\n",
        "\n",
        "        step_feat = original_features[start_idx:end_idx]\n",
        "\n",
        "        if len(step_feat) > 0:\n",
        "            avg_feat = torch.mean(step_feat, dim=0)\n",
        "        else:\n",
        "            avg_feat = torch.zeros_like(original_features[0])\n",
        "\n",
        "        step_embeddings.append(avg_feat.cpu().numpy())\n",
        "\n",
        "    return np.array(step_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rZHCQWa9pqy",
        "outputId": "8eab2495-0fe4-468d-db45-681bf131e6be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading metadata...\n",
            "Loaded parameters for 384 videos.\n",
            "\n",
            "Esempio 1_7: {'n_clusters': 14, 'activity_name': 'Microwave Egg Sandwich', 'activity_id': '1', 'fps': 30.0}\n"
          ]
        }
      ],
      "source": [
        "# @title 1. Load Metadata & Configure Parameters\n",
        "\"\"\"\n",
        "Load video metadata from CSV files and build a parameter dictionary\n",
        "for HiERO processing. Maps each video to the number of recipe steps\n",
        "and frame rate.\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "\n",
        "# --- PATH CONFIGURATION ---\n",
        "# Ensure these paths correctly point to your annotations and task graph files\n",
        "CSV_STEPS_PATH = '/content/drive/MyDrive/AML_Project/annotations-main/annotation_csv/recording_id_step_idx.csv'\n",
        "CSV_NAMES_PATH = '/content/drive/MyDrive/AML_Project/annotations-main/annotation_csv/average_segment_length.csv'\n",
        "CSV_TASK_GRAPHS_PATH = '/content/drive/MyDrive/AML_Project/annotations-main/task_graphs'\n",
        "\n",
        "def load_video_parameters(steps_csv, names_csv):\n",
        "    \"\"\"\n",
        "    Create a dictionary mapping video IDs to their processing parameters.\n",
        "    \n",
        "    For each video, determines:\n",
        "    - n_clusters: The number of unique recipe steps in the task\n",
        "    - activity_name: The name of the recipe\n",
        "    - fps: Frame rate of the video (for temporal scaling)\n",
        "    \n",
        "    This information is loaded from CSV files and task graph JSON files,\n",
        "    providing semantic structure for the clustering process.\n",
        "    \n",
        "    Args:\n",
        "        steps_csv (str): Path to CSV file with step indices for each recording\n",
        "        names_csv (str): Path to CSV file with recipe names and metadata\n",
        "    \n",
        "    Returns:\n",
        "        dict: Mapping from video_id to {n_clusters, activity_name, activity_id, fps}\n",
        "    \n",
        "    Raises:\n",
        "        FileNotFoundError: If CSV files or task graph files cannot be found\n",
        "    \"\"\"\n",
        "    print(\"Loading metadata...\")\n",
        "\n",
        "    # Load CSV files\n",
        "    df_steps = pd.read_csv(steps_csv)\n",
        "    df_names = pd.read_csv(names_csv)\n",
        "\n",
        "    # Ensure IDs are strings for consistent matching\n",
        "    df_steps['activity_id'] = df_steps['activity_id'].astype(str)\n",
        "    df_names['activity_id'] = df_names['activity_id'].astype(str)\n",
        "\n",
        "    # Merge dataframes on activity_id to associate recipe names with recordings\n",
        "    df_merged = pd.merge(df_steps, df_names, on='activity_id', how='left')\n",
        "\n",
        "    video_params = {}\n",
        "\n",
        "    for _, row in df_merged.iterrows():\n",
        "        video_id = str(row['recording_id'])\n",
        "\n",
        "        # --- COMPUTE N_CLUSTERS ---\n",
        "        # Strategy: Number of clusters equals the number of unique steps in the recipe.\n",
        "        # Example: step sequence \"3,1,4,1,3\" -> Unique steps: {1,3,4} -> n_clusters=3\n",
        "        # HiERO will group semantically similar features; temporal boundaries separate repetitions.\n",
        "        try:\n",
        "            step_list = [int(s) for s in str(row['step_indices']).split(',')]\n",
        "            n_unique_steps = len(set(step_list))\n",
        "            s = row['activity_name'].replace(' ', '').lower()\n",
        "            with open(f\"{CSV_TASK_GRAPHS_PATH}/{s}.json\", \"r\") as f:\n",
        "              data = json.load(f)\n",
        "            num_steps = len(data[\"steps\"])\n",
        "            # Safety: minimum 2 clusters required for clustering algorithm\n",
        "            #n_clusters = max(2, n_unique_steps)\n",
        "        except:\n",
        "            print(f\"Warning: Could not parse steps for {video_id}, using default n_clusters=7\")\n",
        "            num_steps = 7\n",
        "\n",
        "        # --- FRAME RATE (FPS) ---\n",
        "        # CSV files do not contain FPS information directly.\n",
        "        # Default value of 30.0 is used; main processing loop refines this from video metadata.\n",
        "        fps = 30.0\n",
        "\n",
        "        video_params[video_id] = {\n",
        "            'n_clusters': num_steps,\n",
        "            'activity_name': row['activity_name'],\n",
        "            'activity_id': row['activity_id'],\n",
        "            'fps': fps\n",
        "        }\n",
        "\n",
        "    print(f\"Loaded parameters for {len(video_params)} videos.\")\n",
        "    return video_params\n",
        "\n",
        "# Execute metadata loading\n",
        "try:\n",
        "    VIDEO_PARAMS = load_video_parameters(CSV_STEPS_PATH, CSV_NAMES_PATH)\n",
        "\n",
        "    # Print sample for verification\n",
        "    sample_id = list(VIDEO_PARAMS.keys())[0]\n",
        "    print(f\"\\nSample video {sample_id}: {VIDEO_PARAMS[sample_id]}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading CSV files: {e}\")\n",
        "    # Fallback to empty dictionary to prevent blocking\n",
        "    VIDEO_PARAMS = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QngYmZZ8MnSv",
        "outputId": "b92e4acb-92a1-4e34-a491-fe7031051433"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading parameters from: /content/drive/MyDrive/AML_Project/Extension/step1_HiERO/video_params_dump.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-727532447.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  weights = torch.load(ckpt_path, map_location=DEVICE)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 768 feature files. Processing matched videos...\n",
            "[OK] 10_16 -> Saved (K=17, Segments=35)\n",
            "[OK] 10_18 -> Saved (K=17, Segments=34)\n",
            "[OK] 10_24 -> Saved (K=17, Segments=40)\n",
            "[OK] 10_26 -> Saved (K=17, Segments=26)\n",
            "[OK] 10_31 -> Saved (K=17, Segments=23)\n",
            "[OK] 10_42 -> Saved (K=17, Segments=30)\n",
            "[OK] 10_46 -> Saved (K=17, Segments=31)\n",
            "[OK] 10_47 -> Saved (K=17, Segments=25)\n",
            "[OK] 10_48 -> Saved (K=17, Segments=31)\n",
            "[OK] 10_50 -> Saved (K=17, Segments=17)\n",
            "[OK] 10_6 -> Saved (K=17, Segments=16)\n",
            "[OK] 10_7 -> Saved (K=17, Segments=32)\n",
            "[OK] 12_10 -> Saved (K=9, Segments=18)\n",
            "[OK] 12_119 -> Saved (K=9, Segments=15)\n",
            "[OK] 12_12 -> Saved (K=9, Segments=21)\n",
            "[OK] 12_13 -> Saved (K=9, Segments=23)\n",
            "[OK] 12_15 -> Saved (K=9, Segments=19)\n",
            "[OK] 12_16 -> Saved (K=9, Segments=16)\n",
            "[OK] 12_17 -> Saved (K=9, Segments=9)\n",
            "[OK] 12_19 -> Saved (K=9, Segments=19)\n",
            "[OK] 12_26 -> Saved (K=9, Segments=14)\n",
            "[OK] 12_2 -> Saved (K=9, Segments=16)\n",
            "[OK] 12_38 -> Saved (K=9, Segments=16)\n",
            "[OK] 12_41 -> Saved (K=9, Segments=23)\n",
            "[OK] 12_43 -> Saved (K=9, Segments=13)\n",
            "[OK] 12_48 -> Saved (K=9, Segments=14)\n",
            "[OK] 12_51 -> Saved (K=9, Segments=11)\n",
            "[OK] 12_5 -> Saved (K=9, Segments=17)\n",
            "[OK] 12_6 -> Saved (K=9, Segments=17)\n",
            "[OK] 12_9 -> Saved (K=9, Segments=23)\n",
            "[OK] 13_12 -> Saved (K=12, Segments=21)\n",
            "[OK] 13_14 -> Saved (K=12, Segments=30)\n",
            "[OK] 13_18 -> Saved (K=12, Segments=47)\n",
            "[OK] 13_20 -> Saved (K=12, Segments=35)\n",
            "[OK] 13_24 -> Saved (K=12, Segments=28)\n",
            "[OK] 13_31 -> Saved (K=12, Segments=23)\n",
            "[OK] 13_32 -> Saved (K=12, Segments=24)\n",
            "[OK] 13_36 -> Saved (K=12, Segments=33)\n",
            "[OK] 13_38 -> Saved (K=12, Segments=26)\n",
            "[OK] 13_41 -> Saved (K=12, Segments=37)\n",
            "[OK] 13_44 -> Saved (K=12, Segments=24)\n",
            "[OK] 13_45 -> Saved (K=12, Segments=27)\n",
            "[OK] 13_5 -> Saved (K=12, Segments=41)\n",
            "[OK] 13_9 -> Saved (K=12, Segments=32)\n",
            "[OK] 15_17 -> Saved (K=19, Segments=37)\n",
            "[OK] 15_18 -> Saved (K=19, Segments=53)\n",
            "[OK] 15_19 -> Saved (K=19, Segments=57)\n",
            "[OK] 15_28 -> Saved (K=19, Segments=42)\n",
            "[OK] 15_29 -> Saved (K=19, Segments=43)\n",
            "[OK] 15_2 -> Saved (K=19, Segments=45)\n",
            "[OK] 15_30 -> Saved (K=19, Segments=31)\n",
            "[OK] 15_33 -> Saved (K=19, Segments=33)\n",
            "[OK] 15_37 -> Saved (K=19, Segments=31)\n",
            "[OK] 15_39 -> Saved (K=19, Segments=43)\n",
            "[OK] 15_41 -> Saved (K=19, Segments=44)\n",
            "[OK] 15_46 -> Saved (K=19, Segments=52)\n",
            "[OK] 15_4 -> Saved (K=19, Segments=57)\n",
            "[OK] 15_5 -> Saved (K=19, Segments=69)\n",
            "[OK] 15_8 -> Saved (K=19, Segments=56)\n",
            "[OK] 16_10 -> Saved (K=23, Segments=55)\n",
            "[OK] 16_17 -> Saved (K=23, Segments=80)\n",
            "[OK] 16_18 -> Saved (K=23, Segments=54)\n",
            "[OK] 16_1 -> Saved (K=23, Segments=45)\n",
            "[OK] 16_20 -> Saved (K=23, Segments=88)\n",
            "[OK] 16_23 -> Saved (K=23, Segments=58)\n",
            "[OK] 16_26 -> Saved (K=23, Segments=44)\n",
            "[OK] 16_27 -> Saved (K=23, Segments=53)\n",
            "[OK] 16_28 -> Saved (K=23, Segments=56)\n",
            "[OK] 16_2 -> Saved (K=23, Segments=55)\n",
            "[OK] 16_35 -> Saved (K=23, Segments=57)\n",
            "[OK] 16_39 -> Saved (K=23, Segments=42)\n",
            "[OK] 16_3 -> Saved (K=23, Segments=87)\n",
            "[OK] 16_40 -> Saved (K=23, Segments=70)\n",
            "[OK] 16_42 -> Saved (K=23, Segments=53)\n",
            "[OK] 16_44 -> Saved (K=23, Segments=47)\n",
            "[OK] 17_10 -> Saved (K=11, Segments=28)\n",
            "[OK] 17_11 -> Saved (K=11, Segments=26)\n",
            "[OK] 17_14 -> Saved (K=11, Segments=26)\n",
            "[OK] 17_15 -> Saved (K=11, Segments=29)\n",
            "[OK] 17_16 -> Saved (K=11, Segments=30)\n",
            "[OK] 17_19 -> Saved (K=11, Segments=30)\n",
            "[OK] 17_20 -> Saved (K=11, Segments=25)\n",
            "[OK] 17_21 -> Saved (K=11, Segments=31)\n",
            "[OK] 17_23 -> Saved (K=11, Segments=33)\n",
            "[OK] 17_28 -> Saved (K=11, Segments=20)\n",
            "[OK] 17_29 -> Saved (K=11, Segments=25)\n",
            "[OK] 17_36 -> Saved (K=11, Segments=19)\n",
            "[OK] 17_37 -> Saved (K=11, Segments=23)\n",
            "[OK] 17_3 -> Saved (K=11, Segments=33)\n",
            "[OK] 17_40 -> Saved (K=11, Segments=26)\n",
            "[OK] 17_43 -> Saved (K=11, Segments=31)\n",
            "[OK] 17_45 -> Saved (K=11, Segments=31)\n",
            "[OK] 17_49 -> Saved (K=11, Segments=30)\n",
            "[OK] 17_5 -> Saved (K=11, Segments=36)\n",
            "[OK] 17_8 -> Saved (K=11, Segments=37)\n",
            "[OK] 18_101 -> Saved (K=13, Segments=24)\n",
            "[OK] 18_11 -> Saved (K=13, Segments=25)\n",
            "[OK] 18_12 -> Saved (K=13, Segments=27)\n",
            "[OK] 18_19 -> Saved (K=13, Segments=28)\n",
            "[OK] 18_24 -> Saved (K=13, Segments=30)\n",
            "[OK] 18_27 -> Saved (K=13, Segments=20)\n",
            "[OK] 18_28 -> Saved (K=13, Segments=29)\n",
            "[OK] 18_2 -> Saved (K=13, Segments=26)\n",
            "[OK] 18_31 -> Saved (K=13, Segments=22)\n",
            "[OK] 18_33 -> Saved (K=13, Segments=22)\n",
            "[OK] 18_3 -> Saved (K=13, Segments=30)\n",
            "[OK] 18_41 -> Saved (K=13, Segments=26)\n",
            "[OK] 18_45 -> Saved (K=13, Segments=26)\n",
            "[OK] 18_49 -> Saved (K=13, Segments=35)\n",
            "[OK] 18_8 -> Saved (K=13, Segments=39)\n",
            "[OK] 1_10 -> Saved (K=12, Segments=21)\n",
            "[OK] 1_136 -> Saved (K=12, Segments=29)\n",
            "[OK] 1_143 -> Saved (K=12, Segments=23)\n",
            "[OK] 1_14 -> Saved (K=12, Segments=30)\n",
            "[OK] 1_19 -> Saved (K=12, Segments=33)\n",
            "[OK] 1_20 -> Saved (K=12, Segments=46)\n",
            "[OK] 1_25 -> Saved (K=12, Segments=28)\n",
            "[OK] 1_28 -> Saved (K=12, Segments=30)\n",
            "[OK] 1_30 -> Saved (K=12, Segments=25)\n",
            "[OK] 1_32 -> Saved (K=12, Segments=20)\n",
            "[OK] 1_33 -> Saved (K=12, Segments=29)\n",
            "[OK] 1_34 -> Saved (K=12, Segments=25)\n",
            "[OK] 1_36 -> Saved (K=11, Segments=24)\n",
            "[OK] 1_37 -> Saved (K=12, Segments=27)\n",
            "[OK] 1_42 -> Saved (K=12, Segments=25)\n",
            "[OK] 1_43 -> Saved (K=12, Segments=24)\n",
            "[OK] 1_49 -> Saved (K=12, Segments=23)\n",
            "[OK] 1_7 -> Saved (K=12, Segments=29)\n",
            "[OK] 20_14 -> Saved (K=17, Segments=60)\n",
            "[OK] 20_16 -> Saved (K=17, Segments=54)\n",
            "[OK] 20_17 -> Saved (K=17, Segments=60)\n",
            "[OK] 20_19 -> Saved (K=17, Segments=49)\n",
            "[OK] 20_22 -> Saved (K=17, Segments=41)\n",
            "[OK] 20_25 -> Saved (K=17, Segments=40)\n",
            "[OK] 20_26 -> Saved (K=17, Segments=44)\n",
            "[OK] 20_29 -> Saved (K=17, Segments=45)\n",
            "[OK] 20_32 -> Saved (K=17, Segments=50)\n",
            "[OK] 20_39 -> Saved (K=17, Segments=37)\n",
            "[OK] 20_44 -> Saved (K=17, Segments=39)\n",
            "[OK] 20_47 -> Saved (K=17, Segments=43)\n",
            "[OK] 20_48 -> Saved (K=17, Segments=40)\n",
            "[OK] 20_9 -> Saved (K=17, Segments=48)\n",
            "[OK] 21_103 -> Saved (K=14, Segments=39)\n",
            "[OK] 21_11 -> Saved (K=14, Segments=44)\n",
            "[OK] 21_14 -> Saved (K=14, Segments=23)\n",
            "[OK] 21_15 -> Saved (K=14, Segments=30)\n",
            "[OK] 21_17 -> Saved (K=14, Segments=31)\n",
            "[OK] 21_19 -> Saved (K=14, Segments=48)\n",
            "[OK] 21_24 -> Saved (K=14, Segments=34)\n",
            "[OK] 21_25 -> Saved (K=14, Segments=36)\n",
            "[OK] 21_28 -> Saved (K=14, Segments=25)\n",
            "[OK] 21_29 -> Saved (K=14, Segments=27)\n",
            "[OK] 21_32 -> Saved (K=14, Segments=26)\n",
            "[OK] 21_37 -> Saved (K=14, Segments=29)\n",
            "[OK] 21_3 -> Saved (K=14, Segments=33)\n",
            "[OK] 21_43 -> Saved (K=14, Segments=34)\n",
            "[OK] 21_44 -> Saved (K=14, Segments=38)\n",
            "[OK] 21_46 -> Saved (K=14, Segments=32)\n",
            "[OK] 21_47 -> Saved (K=14, Segments=30)\n",
            "[OK] 21_50 -> Saved (K=14, Segments=35)\n",
            "[OK] 21_8 -> Saved (K=14, Segments=33)\n",
            "[OK] 22_10 -> Saved (K=15, Segments=49)\n",
            "[OK] 22_137 -> Saved (K=15, Segments=30)\n",
            "[OK] 22_13 -> Saved (K=15, Segments=28)\n",
            "[OK] 22_21 -> Saved (K=15, Segments=25)\n",
            "[OK] 22_24 -> Saved (K=15, Segments=73)\n",
            "[OK] 22_26 -> Saved (K=15, Segments=29)\n",
            "[OK] 22_2 -> Saved (K=15, Segments=31)\n",
            "[OK] 22_30 -> Saved (K=15, Segments=26)\n",
            "[OK] 22_31 -> Saved (K=15, Segments=24)\n",
            "[OK] 22_32 -> Saved (K=15, Segments=33)\n",
            "[OK] 22_37 -> Saved (K=15, Segments=33)\n",
            "[OK] 22_38 -> Saved (K=15, Segments=35)\n",
            "[OK] 22_40 -> Saved (K=15, Segments=36)\n",
            "[OK] 22_41 -> Saved (K=15, Segments=26)\n",
            "[OK] 22_4 -> Saved (K=15, Segments=52)\n",
            "[OK] 22_6 -> Saved (K=15, Segments=23)\n",
            "[OK] 22_8 -> Saved (K=15, Segments=18)\n",
            "[OK] 23_12 -> Saved (K=25, Segments=75)\n",
            "[OK] 23_16 -> Saved (K=25, Segments=47)\n",
            "[OK] 23_17 -> Saved (K=25, Segments=79)\n",
            "[OK] 23_18 -> Saved (K=25, Segments=83)\n",
            "[OK] 23_19 -> Saved (K=25, Segments=68)\n",
            "[OK] 23_1 -> Saved (K=25, Segments=95)\n",
            "[OK] 23_23 -> Saved (K=25, Segments=75)\n",
            "[OK] 23_24 -> Saved (K=25, Segments=62)\n",
            "[OK] 23_26 -> Saved (K=25, Segments=51)\n",
            "[OK] 23_32 -> Saved (K=25, Segments=57)\n",
            "[OK] 23_38 -> Saved (K=25, Segments=55)\n",
            "[OK] 23_39 -> Saved (K=25, Segments=65)\n",
            "[OK] 23_41 -> Saved (K=25, Segments=51)\n",
            "[OK] 23_5 -> Saved (K=25, Segments=75)\n",
            "[OK] 23_6 -> Saved (K=25, Segments=110)\n",
            "[OK] 23_9 -> Saved (K=25, Segments=90)\n",
            "[OK] 25_109 -> Saved (K=19, Segments=69)\n",
            "[OK] 25_11 -> Saved (K=19, Segments=43)\n",
            "[OK] 25_13 -> Saved (K=19, Segments=51)\n",
            "[OK] 25_1 -> Saved (K=19, Segments=46)\n",
            "[OK] 25_22 -> Saved (K=19, Segments=59)\n",
            "[OK] 25_2 -> Saved (K=19, Segments=47)\n",
            "[OK] 25_3 -> Saved (K=19, Segments=54)\n",
            "[OK] 25_40 -> Saved (K=19, Segments=37)\n",
            "[OK] 25_41 -> Saved (K=19, Segments=43)\n",
            "[OK] 25_42 -> Saved (K=19, Segments=50)\n",
            "[OK] 25_48 -> Saved (K=19, Segments=43)\n",
            "[OK] 25_4 -> Saved (K=19, Segments=39)\n",
            "[OK] 25_5 -> Saved (K=19, Segments=59)\n",
            "[OK] 25_6 -> Saved (K=19, Segments=60)\n",
            "[OK] 25_9 -> Saved (K=19, Segments=47)\n",
            "[OK] 26_136 -> Saved (K=20, Segments=37)\n",
            "[OK] 26_17 -> Saved (K=20, Segments=74)\n",
            "[OK] 26_18 -> Saved (K=20, Segments=48)\n",
            "[OK] 26_19 -> Saved (K=20, Segments=60)\n",
            "[OK] 26_20 -> Saved (K=20, Segments=52)\n",
            "[OK] 26_22 -> Saved (K=20, Segments=62)\n",
            "[OK] 26_24 -> Saved (K=20, Segments=72)\n",
            "[OK] 26_29 -> Saved (K=20, Segments=38)\n",
            "[OK] 26_30 -> Saved (K=20, Segments=47)\n",
            "[OK] 26_34 -> Saved (K=20, Segments=42)\n",
            "[OK] 26_36 -> Saved (K=20, Segments=39)\n",
            "[OK] 26_39 -> Saved (K=20, Segments=42)\n",
            "[OK] 26_42 -> Saved (K=20, Segments=54)\n",
            "[OK] 26_46 -> Saved (K=20, Segments=40)\n",
            "[OK] 26_4 -> Saved (K=20, Segments=50)\n",
            "[OK] 26_6 -> Saved (K=20, Segments=53)\n",
            "[OK] 26_7 -> Saved (K=20, Segments=71)\n",
            "[OK] 27_13 -> Saved (K=11, Segments=35)\n",
            "[OK] 27_15 -> Saved (K=11, Segments=23)\n",
            "[OK] 27_17 -> Saved (K=11, Segments=34)\n",
            "[OK] 27_18 -> Saved (K=11, Segments=29)\n",
            "[OK] 27_26 -> Saved (K=11, Segments=21)\n",
            "[OK] 27_29 -> Saved (K=11, Segments=30)\n",
            "[OK] 27_31 -> Saved (K=11, Segments=36)\n",
            "[OK] 27_34 -> Saved (K=11, Segments=25)\n",
            "[OK] 27_37 -> Saved (K=11, Segments=21)\n",
            "[OK] 27_38 -> Saved (K=11, Segments=26)\n",
            "[OK] 27_3 -> Saved (K=11, Segments=28)\n",
            "[OK] 27_45 -> Saved (K=11, Segments=34)\n",
            "[OK] 27_49 -> Saved (K=11, Segments=20)\n",
            "[OK] 27_4 -> Saved (K=11, Segments=27)\n",
            "[OK] 27_9 -> Saved (K=11, Segments=38)\n",
            "[OK] 28_10 -> Saved (K=17, Segments=46)\n",
            "[OK] 28_14 -> Saved (K=17, Segments=36)\n",
            "[OK] 28_16 -> Saved (K=17, Segments=35)\n",
            "[OK] 28_21 -> Saved (K=17, Segments=46)\n",
            "[OK] 28_24 -> Saved (K=17, Segments=42)\n",
            "[OK] 28_25 -> Saved (K=17, Segments=42)\n",
            "[OK] 28_26 -> Saved (K=17, Segments=33)\n",
            "[OK] 28_28 -> Saved (K=17, Segments=31)\n",
            "[OK] 28_29 -> Saved (K=17, Segments=30)\n",
            "[OK] 28_2 -> Saved (K=17, Segments=41)\n",
            "[OK] 28_38 -> Saved (K=17, Segments=29)\n",
            "[OK] 28_3 -> Saved (K=17, Segments=40)\n",
            "[OK] 28_42 -> Saved (K=17, Segments=45)\n",
            "[OK] 28_44 -> Saved (K=17, Segments=33)\n",
            "[OK] 28_45 -> Saved (K=17, Segments=45)\n",
            "[OK] 28_49 -> Saved (K=17, Segments=42)\n",
            "[OK] 28_50 -> Saved (K=17, Segments=34)\n",
            "[OK] 28_7 -> Saved (K=17, Segments=44)\n",
            "[OK] 29_129 -> Saved (K=11, Segments=32)\n",
            "[OK] 29_15 -> Saved (K=11, Segments=35)\n",
            "[OK] 29_17 -> Saved (K=11, Segments=27)\n",
            "[OK] 29_18 -> Saved (K=11, Segments=31)\n",
            "[OK] 29_19 -> Saved (K=11, Segments=34)\n",
            "[OK] 29_22 -> Saved (K=11, Segments=39)\n",
            "[OK] 29_28 -> Saved (K=11, Segments=28)\n",
            "[OK] 29_29 -> Saved (K=11, Segments=22)\n",
            "[OK] 29_32 -> Saved (K=11, Segments=24)\n",
            "[OK] 29_34 -> Saved (K=11, Segments=27)\n",
            "[OK] 29_35 -> Saved (K=11, Segments=29)\n",
            "[OK] 29_37 -> Saved (K=11, Segments=26)\n",
            "[OK] 29_45 -> Saved (K=11, Segments=35)\n",
            "[OK] 29_48 -> Saved (K=11, Segments=31)\n",
            "[OK] 29_49 -> Saved (K=11, Segments=28)\n",
            "[OK] 29_5 -> Saved (K=11, Segments=24)\n",
            "[OK] 29_6 -> Saved (K=11, Segments=48)\n",
            "[OK] 29_7 -> Saved (K=11, Segments=41)\n",
            "[OK] 2_13 -> Saved (K=14, Segments=50)\n",
            "[OK] 2_17 -> Saved (K=14, Segments=43)\n",
            "[OK] 2_19 -> Saved (K=14, Segments=31)\n",
            "[OK] 2_22 -> Saved (K=14, Segments=39)\n",
            "[OK] 2_26 -> Saved (K=14, Segments=41)\n",
            "[OK] 2_28 -> Saved (K=14, Segments=32)\n",
            "[OK] 2_30 -> Saved (K=14, Segments=24)\n",
            "[OK] 2_38 -> Saved (K=14, Segments=36)\n",
            "[OK] 2_3 -> Saved (K=14, Segments=37)\n",
            "[OK] 2_41 -> Saved (K=14, Segments=40)\n",
            "[OK] 2_42 -> Saved (K=14, Segments=46)\n",
            "[OK] 2_46 -> Saved (K=14, Segments=31)\n",
            "[OK] 2_47 -> Saved (K=14, Segments=38)\n",
            "[OK] 2_4 -> Saved (K=14, Segments=29)\n",
            "[OK] 2_5 -> Saved (K=14, Segments=33)\n",
            "[OK] 2_8 -> Saved (K=14, Segments=47)\n",
            "[OK] 3_11 -> Saved (K=14, Segments=30)\n",
            "[OK] 3_12 -> Saved (K=14, Segments=36)\n",
            "[OK] 3_13 -> Saved (K=14, Segments=45)\n",
            "[OK] 3_14 -> Saved (K=14, Segments=36)\n",
            "[OK] 3_18 -> Saved (K=14, Segments=44)\n",
            "[OK] 3_22 -> Saved (K=14, Segments=33)\n",
            "[OK] 3_2 -> Saved (K=14, Segments=33)\n",
            "[OK] 3_34 -> Saved (K=14, Segments=30)\n",
            "[OK] 3_36 -> Saved (K=14, Segments=34)\n",
            "[OK] 3_46 -> Saved (K=14, Segments=39)\n",
            "[OK] 3_49 -> Saved (K=14, Segments=34)\n",
            "[OK] 3_50 -> Saved (K=14, Segments=30)\n",
            "[OK] 3_5 -> Saved (K=14, Segments=31)\n",
            "[OK] 4_122 -> Saved (K=15, Segments=33)\n",
            "[OK] 4_17 -> Saved (K=15, Segments=29)\n",
            "[OK] 4_20 -> Saved (K=15, Segments=30)\n",
            "[OK] 4_22 -> Saved (K=15, Segments=39)\n",
            "[OK] 4_24 -> Saved (K=15, Segments=39)\n",
            "[OK] 4_2 -> Saved (K=15, Segments=30)\n",
            "[OK] 4_30 -> Saved (K=15, Segments=26)\n",
            "[OK] 4_32 -> Saved (K=15, Segments=37)\n",
            "[OK] 4_35 -> Saved (K=15, Segments=23)\n",
            "[OK] 4_36 -> Saved (K=15, Segments=36)\n",
            "[OK] 4_3 -> Saved (K=15, Segments=30)\n",
            "[OK] 4_40 -> Saved (K=15, Segments=27)\n",
            "[OK] 4_43 -> Saved (K=15, Segments=32)\n",
            "[OK] 4_44 -> Saved (K=15, Segments=28)\n",
            "[OK] 4_5 -> Saved (K=15, Segments=26)\n",
            "[OK] 4_7 -> Saved (K=15, Segments=34)\n",
            "[OK] 4_9 -> Saved (K=15, Segments=26)\n",
            "[OK] 5_11 -> Saved (K=16, Segments=37)\n",
            "[OK] 5_15 -> Saved (K=16, Segments=34)\n",
            "[OK] 5_18 -> Saved (K=16, Segments=35)\n",
            "[OK] 5_19 -> Saved (K=16, Segments=31)\n",
            "[OK] 5_22 -> Saved (K=16, Segments=30)\n",
            "[OK] 5_24 -> Saved (K=16, Segments=44)\n",
            "[OK] 5_27 -> Saved (K=16, Segments=40)\n",
            "[OK] 5_28 -> Saved (K=16, Segments=24)\n",
            "[OK] 5_2 -> Saved (K=16, Segments=34)\n",
            "[OK] 5_35 -> Saved (K=16, Segments=38)\n",
            "[OK] 5_37 -> Saved (K=16, Segments=35)\n",
            "[OK] 5_3 -> Saved (K=16, Segments=26)\n",
            "[OK] 5_42 -> Saved (K=16, Segments=29)\n",
            "[OK] 5_44 -> Saved (K=16, Segments=43)\n",
            "[OK] 5_4 -> Saved (K=16, Segments=35)\n",
            "[OK] 7_135 -> Saved (K=11, Segments=41)\n",
            "[OK] 7_18 -> Saved (K=11, Segments=24)\n",
            "[OK] 7_19 -> Saved (K=11, Segments=32)\n",
            "[OK] 7_1 -> Saved (K=11, Segments=23)\n",
            "[OK] 7_24 -> Saved (K=11, Segments=25)\n",
            "[OK] 7_26 -> Saved (K=11, Segments=19)\n",
            "[OK] 7_2 -> Saved (K=11, Segments=19)\n",
            "[OK] 7_30 -> Saved (K=11, Segments=16)\n",
            "[OK] 7_32 -> Saved (K=11, Segments=15)\n",
            "[OK] 7_35 -> Saved (K=11, Segments=20)\n",
            "[OK] 7_38 -> Saved (K=11, Segments=22)\n",
            "[OK] 7_3 -> Saved (K=11, Segments=41)\n",
            "[OK] 7_44 -> Saved (K=11, Segments=20)\n",
            "[OK] 7_48 -> Saved (K=11, Segments=26)\n",
            "[OK] 7_50 -> Saved (K=11, Segments=31)\n",
            "[OK] 7_5 -> Saved (K=11, Segments=26)\n",
            "[OK] 8_11 -> Saved (K=7, Segments=14)\n",
            "[OK] 8_15 -> Saved (K=7, Segments=23)\n",
            "[OK] 8_16 -> Saved (K=7, Segments=14)\n",
            "[OK] 8_19 -> Saved (K=7, Segments=15)\n",
            "[OK] 8_20 -> Saved (K=7, Segments=18)\n",
            "[OK] 8_25 -> Saved (K=7, Segments=17)\n",
            "[OK] 8_26 -> Saved (K=7, Segments=15)\n",
            "[OK] 8_30 -> Saved (K=7, Segments=13)\n",
            "[OK] 8_31 -> Saved (K=7, Segments=13)\n",
            "[OK] 8_33 -> Saved (K=7, Segments=10)\n",
            "[OK] 8_35 -> Saved (K=7, Segments=20)\n",
            "[OK] 8_3 -> Saved (K=7, Segments=20)\n",
            "[OK] 8_40 -> Saved (K=7, Segments=18)\n",
            "[OK] 8_44 -> Saved (K=7, Segments=11)\n",
            "[OK] 8_45 -> Saved (K=7, Segments=16)\n",
            "[OK] 8_50 -> Saved (K=7, Segments=13)\n",
            "[OK] 9_108 -> Saved (K=11, Segments=22)\n",
            "[OK] 9_12 -> Saved (K=11, Segments=25)\n",
            "[OK] 9_13 -> Saved (K=11, Segments=22)\n",
            "[OK] 9_15 -> Saved (K=11, Segments=37)\n",
            "[OK] 9_19 -> Saved (K=11, Segments=36)\n",
            "[OK] 9_22 -> Saved (K=11, Segments=33)\n",
            "[OK] 9_24 -> Saved (K=11, Segments=21)\n",
            "[OK] 9_25 -> Saved (K=11, Segments=26)\n",
            "[OK] 9_2 -> Saved (K=11, Segments=21)\n",
            "[OK] 9_36 -> Saved (K=11, Segments=22)\n",
            "[OK] 9_45 -> Saved (K=11, Segments=15)\n",
            "[OK] 9_47 -> Saved (K=11, Segments=24)\n",
            "[OK] 9_4 -> Saved (K=11, Segments=27)\n",
            "[OK] 9_8 -> Saved (K=11, Segments=24)\n",
            "[OK] 10_16 -> Saved (K=17, Segments=35)\n",
            "[OK] 10_18 -> Saved (K=17, Segments=34)\n",
            "[OK] 10_24 -> Saved (K=17, Segments=40)\n",
            "[OK] 10_26 -> Saved (K=17, Segments=26)\n",
            "[OK] 10_31 -> Saved (K=17, Segments=23)\n",
            "[OK] 10_42 -> Saved (K=17, Segments=30)\n",
            "[OK] 10_46 -> Saved (K=17, Segments=31)\n",
            "[OK] 10_47 -> Saved (K=17, Segments=25)\n",
            "[OK] 10_48 -> Saved (K=17, Segments=31)\n",
            "[OK] 10_50 -> Saved (K=17, Segments=17)\n",
            "[OK] 10_6 -> Saved (K=17, Segments=16)\n",
            "[OK] 10_7 -> Saved (K=17, Segments=32)\n",
            "[OK] 12_10 -> Saved (K=9, Segments=18)\n",
            "[OK] 12_119 -> Saved (K=9, Segments=15)\n",
            "[OK] 12_12 -> Saved (K=9, Segments=21)\n",
            "[OK] 12_13 -> Saved (K=9, Segments=23)\n",
            "[OK] 12_15 -> Saved (K=9, Segments=19)\n",
            "[OK] 12_16 -> Saved (K=9, Segments=16)\n",
            "[OK] 12_17 -> Saved (K=9, Segments=9)\n",
            "[OK] 12_19 -> Saved (K=9, Segments=19)\n",
            "[OK] 12_26 -> Saved (K=9, Segments=14)\n",
            "[OK] 12_2 -> Saved (K=9, Segments=16)\n",
            "[OK] 12_38 -> Saved (K=9, Segments=16)\n",
            "[OK] 12_41 -> Saved (K=9, Segments=23)\n",
            "[OK] 12_43 -> Saved (K=9, Segments=13)\n",
            "[OK] 12_48 -> Saved (K=9, Segments=14)\n",
            "[OK] 12_51 -> Saved (K=9, Segments=11)\n",
            "[OK] 12_5 -> Saved (K=9, Segments=17)\n",
            "[OK] 12_6 -> Saved (K=9, Segments=17)\n",
            "[OK] 12_9 -> Saved (K=9, Segments=23)\n",
            "[OK] 13_12 -> Saved (K=12, Segments=21)\n",
            "[OK] 13_14 -> Saved (K=12, Segments=30)\n",
            "[OK] 13_18 -> Saved (K=12, Segments=47)\n",
            "[OK] 13_20 -> Saved (K=12, Segments=35)\n",
            "[OK] 13_24 -> Saved (K=12, Segments=28)\n",
            "[OK] 13_31 -> Saved (K=12, Segments=23)\n",
            "[OK] 13_32 -> Saved (K=12, Segments=24)\n",
            "[OK] 13_36 -> Saved (K=12, Segments=33)\n",
            "[OK] 13_38 -> Saved (K=12, Segments=26)\n",
            "[OK] 13_41 -> Saved (K=12, Segments=37)\n",
            "[OK] 13_44 -> Saved (K=12, Segments=24)\n",
            "[OK] 13_45 -> Saved (K=12, Segments=27)\n",
            "[OK] 13_5 -> Saved (K=12, Segments=41)\n",
            "[OK] 13_9 -> Saved (K=12, Segments=32)\n",
            "[OK] 15_17 -> Saved (K=19, Segments=37)\n",
            "[OK] 15_18 -> Saved (K=19, Segments=53)\n",
            "[OK] 15_19 -> Saved (K=19, Segments=57)\n",
            "[OK] 15_28 -> Saved (K=19, Segments=42)\n",
            "[OK] 15_29 -> Saved (K=19, Segments=43)\n",
            "[OK] 15_2 -> Saved (K=19, Segments=45)\n",
            "[OK] 15_30 -> Saved (K=19, Segments=31)\n",
            "[OK] 15_33 -> Saved (K=19, Segments=33)\n",
            "[OK] 15_37 -> Saved (K=19, Segments=31)\n",
            "[OK] 15_39 -> Saved (K=19, Segments=43)\n",
            "[OK] 15_41 -> Saved (K=19, Segments=44)\n",
            "[OK] 15_46 -> Saved (K=19, Segments=52)\n",
            "[OK] 15_4 -> Saved (K=19, Segments=57)\n",
            "[OK] 15_5 -> Saved (K=19, Segments=69)\n",
            "[OK] 15_8 -> Saved (K=19, Segments=56)\n",
            "[OK] 16_10 -> Saved (K=23, Segments=55)\n",
            "[OK] 16_17 -> Saved (K=23, Segments=80)\n",
            "[OK] 16_18 -> Saved (K=23, Segments=54)\n",
            "[OK] 16_1 -> Saved (K=23, Segments=45)\n",
            "[OK] 16_20 -> Saved (K=23, Segments=88)\n",
            "[OK] 16_23 -> Saved (K=23, Segments=58)\n",
            "[OK] 16_26 -> Saved (K=23, Segments=44)\n",
            "[OK] 16_27 -> Saved (K=23, Segments=53)\n",
            "[OK] 16_28 -> Saved (K=23, Segments=56)\n",
            "[OK] 16_2 -> Saved (K=23, Segments=55)\n",
            "[OK] 16_35 -> Saved (K=23, Segments=57)\n",
            "[OK] 16_39 -> Saved (K=23, Segments=42)\n",
            "[OK] 16_3 -> Saved (K=23, Segments=87)\n",
            "[OK] 16_40 -> Saved (K=23, Segments=70)\n",
            "[OK] 16_42 -> Saved (K=23, Segments=53)\n",
            "[OK] 16_44 -> Saved (K=23, Segments=47)\n",
            "[OK] 17_10 -> Saved (K=11, Segments=28)\n",
            "[OK] 17_11 -> Saved (K=11, Segments=26)\n",
            "[OK] 17_14 -> Saved (K=11, Segments=26)\n",
            "[OK] 17_15 -> Saved (K=11, Segments=29)\n",
            "[OK] 17_16 -> Saved (K=11, Segments=30)\n",
            "[OK] 17_19 -> Saved (K=11, Segments=30)\n",
            "[OK] 17_20 -> Saved (K=11, Segments=25)\n",
            "[OK] 17_21 -> Saved (K=11, Segments=31)\n",
            "[OK] 17_23 -> Saved (K=11, Segments=33)\n",
            "[OK] 17_28 -> Saved (K=11, Segments=20)\n",
            "[OK] 17_29 -> Saved (K=11, Segments=25)\n",
            "[OK] 17_36 -> Saved (K=11, Segments=19)\n",
            "[OK] 17_37 -> Saved (K=11, Segments=23)\n",
            "[OK] 17_3 -> Saved (K=11, Segments=33)\n",
            "[OK] 17_40 -> Saved (K=11, Segments=26)\n",
            "[OK] 17_43 -> Saved (K=11, Segments=31)\n",
            "[OK] 17_45 -> Saved (K=11, Segments=31)\n",
            "[OK] 17_49 -> Saved (K=11, Segments=30)\n",
            "[OK] 17_5 -> Saved (K=11, Segments=36)\n",
            "[OK] 17_8 -> Saved (K=11, Segments=37)\n",
            "[OK] 18_101 -> Saved (K=13, Segments=24)\n",
            "[OK] 18_11 -> Saved (K=13, Segments=25)\n",
            "[OK] 18_12 -> Saved (K=13, Segments=27)\n",
            "[OK] 18_19 -> Saved (K=13, Segments=28)\n",
            "[OK] 18_24 -> Saved (K=13, Segments=30)\n",
            "[OK] 18_27 -> Saved (K=13, Segments=20)\n",
            "[OK] 18_28 -> Saved (K=13, Segments=29)\n",
            "[OK] 18_2 -> Saved (K=13, Segments=26)\n",
            "[OK] 18_31 -> Saved (K=13, Segments=22)\n",
            "[OK] 18_33 -> Saved (K=13, Segments=22)\n",
            "[OK] 18_3 -> Saved (K=13, Segments=30)\n",
            "[OK] 18_41 -> Saved (K=13, Segments=26)\n",
            "[OK] 18_45 -> Saved (K=13, Segments=26)\n",
            "[OK] 18_49 -> Saved (K=13, Segments=35)\n",
            "[OK] 18_8 -> Saved (K=13, Segments=39)\n",
            "[OK] 1_10 -> Saved (K=12, Segments=21)\n",
            "[OK] 1_136 -> Saved (K=12, Segments=29)\n",
            "[OK] 1_143 -> Saved (K=12, Segments=23)\n",
            "[OK] 1_14 -> Saved (K=12, Segments=30)\n",
            "[OK] 1_19 -> Saved (K=12, Segments=33)\n",
            "[OK] 1_20 -> Saved (K=12, Segments=46)\n",
            "[OK] 1_25 -> Saved (K=12, Segments=28)\n",
            "[OK] 1_28 -> Saved (K=12, Segments=30)\n",
            "[OK] 1_30 -> Saved (K=12, Segments=25)\n",
            "[OK] 1_32 -> Saved (K=12, Segments=20)\n",
            "[OK] 1_33 -> Saved (K=12, Segments=29)\n",
            "[OK] 1_34 -> Saved (K=12, Segments=25)\n",
            "[OK] 1_36 -> Saved (K=11, Segments=24)\n",
            "[OK] 1_37 -> Saved (K=12, Segments=27)\n",
            "[OK] 1_42 -> Saved (K=12, Segments=25)\n",
            "[OK] 1_43 -> Saved (K=12, Segments=24)\n",
            "[OK] 1_49 -> Saved (K=12, Segments=23)\n",
            "[OK] 1_7 -> Saved (K=12, Segments=29)\n",
            "[OK] 20_14 -> Saved (K=17, Segments=60)\n",
            "[OK] 20_16 -> Saved (K=17, Segments=54)\n",
            "[OK] 20_17 -> Saved (K=17, Segments=60)\n",
            "[OK] 20_19 -> Saved (K=17, Segments=49)\n",
            "[OK] 20_22 -> Saved (K=17, Segments=41)\n",
            "[OK] 20_25 -> Saved (K=17, Segments=40)\n",
            "[OK] 20_26 -> Saved (K=17, Segments=44)\n",
            "[OK] 20_29 -> Saved (K=17, Segments=45)\n",
            "[OK] 20_32 -> Saved (K=17, Segments=50)\n",
            "[OK] 20_39 -> Saved (K=17, Segments=37)\n",
            "[OK] 20_44 -> Saved (K=17, Segments=39)\n",
            "[OK] 20_47 -> Saved (K=17, Segments=43)\n",
            "[OK] 20_48 -> Saved (K=17, Segments=40)\n",
            "[OK] 20_9 -> Saved (K=17, Segments=48)\n",
            "[OK] 21_103 -> Saved (K=14, Segments=39)\n",
            "[OK] 21_11 -> Saved (K=14, Segments=44)\n",
            "[OK] 21_14 -> Saved (K=14, Segments=23)\n",
            "[OK] 21_15 -> Saved (K=14, Segments=30)\n",
            "[OK] 21_17 -> Saved (K=14, Segments=31)\n",
            "[OK] 21_19 -> Saved (K=14, Segments=48)\n",
            "[OK] 21_24 -> Saved (K=14, Segments=34)\n",
            "[OK] 21_25 -> Saved (K=14, Segments=36)\n",
            "[OK] 21_28 -> Saved (K=14, Segments=25)\n",
            "[OK] 21_29 -> Saved (K=14, Segments=27)\n",
            "[OK] 21_32 -> Saved (K=14, Segments=26)\n",
            "[OK] 21_37 -> Saved (K=14, Segments=29)\n",
            "[OK] 21_3 -> Saved (K=14, Segments=33)\n",
            "[OK] 21_43 -> Saved (K=14, Segments=34)\n",
            "[OK] 21_44 -> Saved (K=14, Segments=38)\n",
            "[OK] 21_46 -> Saved (K=14, Segments=32)\n",
            "[OK] 21_47 -> Saved (K=14, Segments=30)\n",
            "[OK] 21_50 -> Saved (K=14, Segments=35)\n",
            "[OK] 21_8 -> Saved (K=14, Segments=33)\n",
            "[OK] 22_10 -> Saved (K=15, Segments=49)\n",
            "[OK] 22_137 -> Saved (K=15, Segments=30)\n",
            "[OK] 22_13 -> Saved (K=15, Segments=28)\n",
            "[OK] 22_21 -> Saved (K=15, Segments=25)\n",
            "[OK] 22_24 -> Saved (K=15, Segments=73)\n",
            "[OK] 22_26 -> Saved (K=15, Segments=29)\n",
            "[OK] 22_2 -> Saved (K=15, Segments=31)\n",
            "[OK] 22_30 -> Saved (K=15, Segments=26)\n",
            "[OK] 22_31 -> Saved (K=15, Segments=24)\n",
            "[OK] 22_32 -> Saved (K=15, Segments=33)\n",
            "[OK] 22_37 -> Saved (K=15, Segments=33)\n",
            "[OK] 22_38 -> Saved (K=15, Segments=35)\n",
            "[OK] 22_40 -> Saved (K=15, Segments=36)\n",
            "[OK] 22_41 -> Saved (K=15, Segments=26)\n",
            "[OK] 22_4 -> Saved (K=15, Segments=52)\n",
            "[OK] 22_6 -> Saved (K=15, Segments=23)\n",
            "[OK] 22_8 -> Saved (K=15, Segments=18)\n",
            "[OK] 23_12 -> Saved (K=25, Segments=75)\n",
            "[OK] 23_16 -> Saved (K=25, Segments=47)\n",
            "[OK] 23_17 -> Saved (K=25, Segments=79)\n",
            "[OK] 23_18 -> Saved (K=25, Segments=83)\n",
            "[OK] 23_19 -> Saved (K=25, Segments=68)\n",
            "[OK] 23_1 -> Saved (K=25, Segments=95)\n",
            "[OK] 23_23 -> Saved (K=25, Segments=75)\n",
            "[OK] 23_24 -> Saved (K=25, Segments=62)\n",
            "[OK] 23_26 -> Saved (K=25, Segments=51)\n",
            "[OK] 23_32 -> Saved (K=25, Segments=57)\n",
            "[OK] 23_38 -> Saved (K=25, Segments=55)\n",
            "[OK] 23_39 -> Saved (K=25, Segments=65)\n",
            "[OK] 23_41 -> Saved (K=25, Segments=51)\n",
            "[OK] 23_5 -> Saved (K=25, Segments=75)\n",
            "[OK] 23_6 -> Saved (K=25, Segments=110)\n",
            "[OK] 23_9 -> Saved (K=25, Segments=90)\n",
            "[OK] 25_109 -> Saved (K=19, Segments=69)\n",
            "[OK] 25_11 -> Saved (K=19, Segments=43)\n",
            "[OK] 25_13 -> Saved (K=19, Segments=51)\n",
            "[OK] 25_1 -> Saved (K=19, Segments=46)\n",
            "[OK] 25_22 -> Saved (K=19, Segments=59)\n",
            "[OK] 25_2 -> Saved (K=19, Segments=47)\n",
            "[OK] 25_3 -> Saved (K=19, Segments=54)\n",
            "[OK] 25_40 -> Saved (K=19, Segments=37)\n",
            "[OK] 25_41 -> Saved (K=19, Segments=43)\n",
            "[OK] 25_42 -> Saved (K=19, Segments=50)\n",
            "[OK] 25_48 -> Saved (K=19, Segments=43)\n",
            "[OK] 25_4 -> Saved (K=19, Segments=39)\n",
            "[OK] 25_5 -> Saved (K=19, Segments=59)\n",
            "[OK] 25_6 -> Saved (K=19, Segments=60)\n",
            "[OK] 25_9 -> Saved (K=19, Segments=47)\n",
            "[OK] 26_136 -> Saved (K=20, Segments=37)\n",
            "[OK] 26_17 -> Saved (K=20, Segments=74)\n",
            "[OK] 26_18 -> Saved (K=20, Segments=48)\n",
            "[OK] 26_19 -> Saved (K=20, Segments=60)\n",
            "[OK] 26_20 -> Saved (K=20, Segments=52)\n",
            "[OK] 26_22 -> Saved (K=20, Segments=62)\n",
            "[OK] 26_24 -> Saved (K=20, Segments=72)\n",
            "[OK] 26_29 -> Saved (K=20, Segments=38)\n",
            "[OK] 26_30 -> Saved (K=20, Segments=47)\n",
            "[OK] 26_34 -> Saved (K=20, Segments=42)\n",
            "[OK] 26_36 -> Saved (K=20, Segments=39)\n",
            "[OK] 26_39 -> Saved (K=20, Segments=42)\n",
            "[OK] 26_42 -> Saved (K=20, Segments=54)\n",
            "[OK] 26_46 -> Saved (K=20, Segments=40)\n",
            "[OK] 26_4 -> Saved (K=20, Segments=50)\n",
            "[OK] 26_6 -> Saved (K=20, Segments=53)\n",
            "[OK] 26_7 -> Saved (K=20, Segments=71)\n",
            "[OK] 27_13 -> Saved (K=11, Segments=35)\n",
            "[OK] 27_15 -> Saved (K=11, Segments=23)\n",
            "[OK] 27_17 -> Saved (K=11, Segments=34)\n",
            "[OK] 27_18 -> Saved (K=11, Segments=29)\n",
            "[OK] 27_26 -> Saved (K=11, Segments=21)\n",
            "[OK] 27_29 -> Saved (K=11, Segments=30)\n",
            "[OK] 27_31 -> Saved (K=11, Segments=36)\n",
            "[OK] 27_34 -> Saved (K=11, Segments=25)\n",
            "[OK] 27_37 -> Saved (K=11, Segments=21)\n",
            "[OK] 27_38 -> Saved (K=11, Segments=26)\n",
            "[OK] 27_3 -> Saved (K=11, Segments=28)\n",
            "[OK] 27_45 -> Saved (K=11, Segments=34)\n",
            "[OK] 27_49 -> Saved (K=11, Segments=20)\n",
            "[OK] 27_4 -> Saved (K=11, Segments=27)\n",
            "[OK] 27_9 -> Saved (K=11, Segments=38)\n",
            "[OK] 28_10 -> Saved (K=17, Segments=46)\n",
            "[OK] 28_14 -> Saved (K=17, Segments=36)\n",
            "[OK] 28_16 -> Saved (K=17, Segments=35)\n",
            "[OK] 28_21 -> Saved (K=17, Segments=46)\n",
            "[OK] 28_24 -> Saved (K=17, Segments=42)\n",
            "[OK] 28_25 -> Saved (K=17, Segments=42)\n",
            "[OK] 28_26 -> Saved (K=17, Segments=33)\n",
            "[OK] 28_28 -> Saved (K=17, Segments=31)\n",
            "[OK] 28_29 -> Saved (K=17, Segments=30)\n",
            "[OK] 28_2 -> Saved (K=17, Segments=41)\n",
            "[OK] 28_38 -> Saved (K=17, Segments=29)\n",
            "[OK] 28_3 -> Saved (K=17, Segments=40)\n",
            "[OK] 28_42 -> Saved (K=17, Segments=45)\n",
            "[OK] 28_44 -> Saved (K=17, Segments=33)\n",
            "[OK] 28_45 -> Saved (K=17, Segments=45)\n",
            "[OK] 28_49 -> Saved (K=17, Segments=42)\n",
            "[OK] 28_50 -> Saved (K=17, Segments=34)\n",
            "[OK] 28_7 -> Saved (K=17, Segments=44)\n",
            "[OK] 29_129 -> Saved (K=11, Segments=32)\n",
            "[OK] 29_15 -> Saved (K=11, Segments=35)\n",
            "[OK] 29_17 -> Saved (K=11, Segments=27)\n",
            "[OK] 29_18 -> Saved (K=11, Segments=31)\n",
            "[OK] 29_19 -> Saved (K=11, Segments=34)\n",
            "[OK] 29_22 -> Saved (K=11, Segments=39)\n",
            "[OK] 29_28 -> Saved (K=11, Segments=28)\n",
            "[OK] 29_29 -> Saved (K=11, Segments=22)\n",
            "[OK] 29_32 -> Saved (K=11, Segments=24)\n",
            "[OK] 29_34 -> Saved (K=11, Segments=27)\n",
            "[OK] 29_35 -> Saved (K=11, Segments=29)\n",
            "[OK] 29_37 -> Saved (K=11, Segments=26)\n",
            "[OK] 29_45 -> Saved (K=11, Segments=35)\n",
            "[OK] 29_48 -> Saved (K=11, Segments=31)\n",
            "[OK] 29_49 -> Saved (K=11, Segments=28)\n",
            "[OK] 29_5 -> Saved (K=11, Segments=24)\n",
            "[OK] 29_6 -> Saved (K=11, Segments=48)\n",
            "[OK] 29_7 -> Saved (K=11, Segments=41)\n",
            "[OK] 2_13 -> Saved (K=14, Segments=50)\n",
            "[OK] 2_17 -> Saved (K=14, Segments=43)\n",
            "[OK] 2_19 -> Saved (K=14, Segments=31)\n",
            "[OK] 2_22 -> Saved (K=14, Segments=39)\n",
            "[OK] 2_26 -> Saved (K=14, Segments=41)\n",
            "[OK] 2_28 -> Saved (K=14, Segments=32)\n",
            "[OK] 2_30 -> Saved (K=14, Segments=24)\n",
            "[OK] 2_38 -> Saved (K=14, Segments=36)\n",
            "[OK] 2_3 -> Saved (K=14, Segments=37)\n",
            "[OK] 2_41 -> Saved (K=14, Segments=40)\n",
            "[OK] 2_42 -> Saved (K=14, Segments=46)\n",
            "[OK] 2_46 -> Saved (K=14, Segments=31)\n",
            "[OK] 2_47 -> Saved (K=14, Segments=38)\n",
            "[OK] 2_4 -> Saved (K=14, Segments=29)\n",
            "[OK] 2_5 -> Saved (K=14, Segments=33)\n",
            "[OK] 2_8 -> Saved (K=14, Segments=47)\n",
            "[OK] 3_11 -> Saved (K=14, Segments=30)\n",
            "[OK] 3_12 -> Saved (K=14, Segments=36)\n",
            "[OK] 3_13 -> Saved (K=14, Segments=45)\n",
            "[OK] 3_14 -> Saved (K=14, Segments=36)\n",
            "[OK] 3_18 -> Saved (K=14, Segments=44)\n",
            "[OK] 3_22 -> Saved (K=14, Segments=33)\n",
            "[OK] 3_2 -> Saved (K=14, Segments=33)\n",
            "[OK] 3_34 -> Saved (K=14, Segments=30)\n",
            "[OK] 3_36 -> Saved (K=14, Segments=34)\n",
            "[OK] 3_46 -> Saved (K=14, Segments=39)\n",
            "[OK] 3_49 -> Saved (K=14, Segments=34)\n",
            "[OK] 3_50 -> Saved (K=14, Segments=30)\n",
            "[OK] 3_5 -> Saved (K=14, Segments=31)\n",
            "[OK] 4_122 -> Saved (K=15, Segments=33)\n",
            "[OK] 4_17 -> Saved (K=15, Segments=29)\n",
            "[OK] 4_20 -> Saved (K=15, Segments=30)\n",
            "[OK] 4_22 -> Saved (K=15, Segments=39)\n",
            "[OK] 4_24 -> Saved (K=15, Segments=39)\n",
            "[OK] 4_2 -> Saved (K=15, Segments=30)\n",
            "[OK] 4_30 -> Saved (K=15, Segments=26)\n",
            "[OK] 4_32 -> Saved (K=15, Segments=37)\n",
            "[OK] 4_35 -> Saved (K=15, Segments=23)\n",
            "[OK] 4_36 -> Saved (K=15, Segments=36)\n",
            "[OK] 4_3 -> Saved (K=15, Segments=30)\n",
            "[OK] 4_40 -> Saved (K=15, Segments=27)\n",
            "[OK] 4_43 -> Saved (K=15, Segments=32)\n",
            "[OK] 4_44 -> Saved (K=15, Segments=28)\n",
            "[OK] 4_5 -> Saved (K=15, Segments=26)\n",
            "[OK] 4_7 -> Saved (K=15, Segments=34)\n",
            "[OK] 4_9 -> Saved (K=15, Segments=26)\n",
            "[OK] 5_11 -> Saved (K=16, Segments=37)\n",
            "[OK] 5_15 -> Saved (K=16, Segments=34)\n",
            "[OK] 5_18 -> Saved (K=16, Segments=35)\n",
            "[OK] 5_19 -> Saved (K=16, Segments=31)\n",
            "[OK] 5_22 -> Saved (K=16, Segments=30)\n",
            "[OK] 5_24 -> Saved (K=16, Segments=44)\n",
            "[OK] 5_27 -> Saved (K=16, Segments=40)\n",
            "[OK] 5_28 -> Saved (K=16, Segments=24)\n",
            "[OK] 5_2 -> Saved (K=16, Segments=34)\n",
            "[OK] 5_35 -> Saved (K=16, Segments=38)\n",
            "[OK] 5_37 -> Saved (K=16, Segments=35)\n",
            "[OK] 5_3 -> Saved (K=16, Segments=26)\n",
            "[OK] 5_42 -> Saved (K=16, Segments=29)\n",
            "[OK] 5_44 -> Saved (K=16, Segments=43)\n",
            "[OK] 5_4 -> Saved (K=16, Segments=35)\n",
            "[OK] 7_135 -> Saved (K=11, Segments=41)\n",
            "[OK] 7_18 -> Saved (K=11, Segments=24)\n",
            "[OK] 7_19 -> Saved (K=11, Segments=32)\n",
            "[OK] 7_1 -> Saved (K=11, Segments=23)\n",
            "[OK] 7_24 -> Saved (K=11, Segments=25)\n",
            "[OK] 7_26 -> Saved (K=11, Segments=19)\n",
            "[OK] 7_2 -> Saved (K=11, Segments=19)\n",
            "[OK] 7_30 -> Saved (K=11, Segments=16)\n",
            "[OK] 7_32 -> Saved (K=11, Segments=15)\n",
            "[OK] 7_35 -> Saved (K=11, Segments=20)\n",
            "[OK] 7_38 -> Saved (K=11, Segments=22)\n",
            "[OK] 7_3 -> Saved (K=11, Segments=41)\n",
            "[OK] 7_44 -> Saved (K=11, Segments=20)\n",
            "[OK] 7_48 -> Saved (K=11, Segments=26)\n",
            "[OK] 7_50 -> Saved (K=11, Segments=31)\n",
            "[OK] 7_5 -> Saved (K=11, Segments=26)\n",
            "[OK] 8_11 -> Saved (K=7, Segments=14)\n",
            "[OK] 8_15 -> Saved (K=7, Segments=23)\n",
            "[OK] 8_16 -> Saved (K=7, Segments=14)\n",
            "[OK] 8_19 -> Saved (K=7, Segments=15)\n",
            "[OK] 8_20 -> Saved (K=7, Segments=18)\n",
            "[OK] 8_25 -> Saved (K=7, Segments=17)\n",
            "[OK] 8_26 -> Saved (K=7, Segments=15)\n",
            "[OK] 8_30 -> Saved (K=7, Segments=13)\n",
            "[OK] 8_31 -> Saved (K=7, Segments=13)\n",
            "[OK] 8_33 -> Saved (K=7, Segments=10)\n",
            "[OK] 8_35 -> Saved (K=7, Segments=20)\n",
            "[OK] 8_3 -> Saved (K=7, Segments=20)\n",
            "[OK] 8_40 -> Saved (K=7, Segments=18)\n",
            "[OK] 8_44 -> Saved (K=7, Segments=11)\n",
            "[OK] 8_45 -> Saved (K=7, Segments=16)\n",
            "[OK] 8_50 -> Saved (K=7, Segments=13)\n",
            "[OK] 9_108 -> Saved (K=11, Segments=22)\n",
            "[OK] 9_12 -> Saved (K=11, Segments=25)\n",
            "[OK] 9_13 -> Saved (K=11, Segments=22)\n",
            "[OK] 9_15 -> Saved (K=11, Segments=37)\n",
            "[OK] 9_19 -> Saved (K=11, Segments=36)\n",
            "[OK] 9_22 -> Saved (K=11, Segments=33)\n",
            "[OK] 9_24 -> Saved (K=11, Segments=21)\n",
            "[OK] 9_25 -> Saved (K=11, Segments=26)\n",
            "[OK] 9_2 -> Saved (K=11, Segments=21)\n",
            "[OK] 9_36 -> Saved (K=11, Segments=22)\n",
            "[OK] 9_45 -> Saved (K=11, Segments=15)\n",
            "[OK] 9_47 -> Saved (K=11, Segments=24)\n",
            "[OK] 9_4 -> Saved (K=11, Segments=27)\n",
            "[OK] 9_8 -> Saved (K=11, Segments=24)\n",
            "\n",
            "Done. Processed: 768, Skipped: 0\n"
          ]
        }
      ],
      "source": [
        "# @title HiERO Step Localization (Robust Matching Version + Median Filter)\n",
        "\"\"\"\n",
        "Main processing pipeline for recipe step localization using HiERO.\n",
        "\n",
        "Workflow:\n",
        "1. Load hierarchical features from HiERO model inference\n",
        "2. Apply spectral clustering to identify temporal clusters\n",
        "3. Filter noise using median filtering on cluster labels\n",
        "4. Convert clusters to temporal step boundaries\n",
        "5. Compute step-level embeddings by averaging original features\n",
        "6. Save results (segments, embeddings, labels) per video\n",
        "\"\"\"\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import hydra\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from torch.nn import functional as F\n",
        "from scipy.ndimage import median_filter  # Temporal smoothing filter\n",
        "\n",
        "# ==========================================\n",
        "# 1. CONFIGURATION\n",
        "# ==========================================\n",
        "\n",
        "# Input/Output paths\n",
        "INPUT_DIR_FEAT = '/content/drive/MyDrive/AML_Project/3_EgoVLP/features'\n",
        "OUTPUT_DIR     = '/content/drive/MyDrive/AML_Project/Extension/step1_HiERO/steps_v4'\n",
        "PARAMS_CSV     = '/content/drive/MyDrive/AML_Project/Extension/step1_HiERO/video_params_dump.csv'\n",
        "CKPT_PATH      = '/content/drive/MyDrive/AML_Project/Extension/step1_HiERO/HiERO/checkpoints/hiero_egovlp.pth'\n",
        "\n",
        "# Fixed HiERO parameters\n",
        "STRIDE = 16  # Frame stride in feature extraction (affects temporal resolution)\n",
        "DEPTH = 2    # Hierarchical depth for feature extraction\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ==========================================\n",
        "# 2. CORE FUNCTIONS\n",
        "# ==========================================\n",
        "\n",
        "def load_params_from_dump(csv_path):\n",
        "    \"\"\"\n",
        "    Load video processing parameters (n_clusters and fps) from CSV dump file.\n",
        "    \n",
        "    Args:\n",
        "        csv_path (str): Path to the parameters CSV file\n",
        "    \n",
        "    Returns:\n",
        "        dict: Mapping from video_id to {n_clusters, fps}\n",
        "    \n",
        "    Raises:\n",
        "        FileNotFoundError: If CSV file does not exist\n",
        "    \"\"\"\n",
        "    print(f\"Loading parameters from: {csv_path}\")\n",
        "    if not os.path.exists(csv_path):\n",
        "        raise FileNotFoundError(f\"CSV not found: {csv_path}\")\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Ensure video_id is string and whitespace-trimmed\n",
        "    df['video_id'] = df['video_id'].astype(str).str.strip()\n",
        "\n",
        "    # Build fast lookup dictionary: id -> {n_clusters, fps}\n",
        "    params_map = {}\n",
        "    for _, row in df.iterrows():\n",
        "        # Safe fps fallback to 30.0 if missing\n",
        "        fps_val = float(row['fps']) if 'fps' in row and not pd.isna(row['fps']) else 30.0\n",
        "\n",
        "        params_map[row['video_id']] = {\n",
        "            'n_clusters': int(row['n_clusters']),\n",
        "            'fps': fps_val\n",
        "        }\n",
        "    return params_map\n",
        "\n",
        "def find_matching_id(filename, valid_ids):\n",
        "    \"\"\"\n",
        "    Find which valid video ID is contained in the feature filename.\n",
        "    \n",
        "    Attempts multiple matching strategies (exact match, prefix match)\n",
        "    to robustly identify the video ID from filename.\n",
        "    \n",
        "    Args:\n",
        "        filename (str): Feature file name\n",
        "        valid_ids (set): Set of valid video IDs from parameters\n",
        "    \n",
        "    Returns:\n",
        "        str or None: Matched video ID, or None if no match found\n",
        "    \"\"\"\n",
        "    clean_name = os.path.splitext(filename)[0]\n",
        "    if clean_name in valid_ids:\n",
        "        return clean_name\n",
        "\n",
        "    # Try prefix matching\n",
        "    candidates = []\n",
        "    for vid in valid_ids:\n",
        "        if filename.startswith(vid + \"_\") or filename.startswith(vid + \".\"):\n",
        "            candidates.append(vid)\n",
        "\n",
        "    if candidates:\n",
        "        return max(candidates, key=len)\n",
        "\n",
        "    return None\n",
        "\n",
        "def build_hiero_model(ckpt_path):\n",
        "    \"\"\"\n",
        "    Load HiERO model from checkpoint (wrapper for utility function).\n",
        "    \n",
        "    Args:\n",
        "        ckpt_path (str): Path to model checkpoint\n",
        "    \n",
        "    Returns:\n",
        "        torch.nn.Module: Loaded model in evaluation mode\n",
        "    \"\"\"\n",
        "    weights = torch.load(ckpt_path, map_location=DEVICE)\n",
        "    model = hydra.utils.instantiate(weights[\"config\"][\"model\"], clustering_at_inference=True, input_size=256, _recursive_=False).to(DEVICE)\n",
        "    model.load_state_dict(weights[\"model\"], strict=False)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def extract_hiero_features(model, features):\n",
        "    \"\"\"\n",
        "    Extract hierarchical features using HiERO model (wrapper for utility function).\n",
        "    \n",
        "    Args:\n",
        "        model: HiERO model instance\n",
        "        features (torch.Tensor): Input features (T, feature_dim)\n",
        "    \n",
        "    Returns:\n",
        "        torch.Tensor: Hierarchical features at depth=DEPTH\n",
        "    \"\"\"\n",
        "    features = features.to(DEVICE)\n",
        "    pos = torch.arange(0, features.shape[0], device=DEVICE).float()\n",
        "    indices = torch.arange(0, features.shape[0], device=DEVICE)\n",
        "    batch = torch.zeros_like(indices, dtype=torch.long)\n",
        "    mask = torch.ones_like(indices, dtype=torch.bool)\n",
        "    data = Data(x=features.unsqueeze(1), pos=pos, indices=indices, batch=batch, mask=mask)\n",
        "    with torch.no_grad():\n",
        "        graphs = model(data)\n",
        "        return graphs.x[graphs.depth == DEPTH]\n",
        "\n",
        "def cluster_and_segment(features, n_clusters, fps):\n",
        "    \"\"\"\n",
        "    Perform spectral clustering and convert cluster labels to temporal segments.\n",
        "    \n",
        "    Applies median filtering to the cluster labels to reduce noise and ensure\n",
        "    temporal stability of segment boundaries.\n",
        "    \n",
        "    Args:\n",
        "        features (torch.Tensor): Hierarchical features to cluster\n",
        "        n_clusters (int): Target number of recipe steps\n",
        "        fps (float): Frame rate for temporal scaling\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (segments, labels) where:\n",
        "            - segments: List of (start_time, end_time) tuples\n",
        "            - labels: Filtered cluster labels array\n",
        "    \"\"\"\n",
        "    features_norm = F.normalize(features, p=2, dim=-1)\n",
        "    affinity = torch.exp((features_norm @ features_norm.T) / 0.05).cpu().numpy()\n",
        "    assign_labels = 'discretize' if n_clusters < 3 else 'kmeans'\n",
        "\n",
        "    # 1. Perform spectral clustering\n",
        "    sc = SpectralClustering(n_clusters=n_clusters, affinity=\"precomputed\", assign_labels=assign_labels, random_state=42)\n",
        "    labels = sc.fit_predict(affinity)\n",
        "\n",
        "    # ============================================================\n",
        "    # MEDIAN FILTERING FOR TEMPORAL SMOOTHING\n",
        "    # ============================================================\n",
        "    # Kernel size=5 reduces noise and prevents overly fragmented segments.\n",
        "    # Ensures temporal stability of cluster labels before computing boundaries.\n",
        "    if len(labels) > 0:\n",
        "        labels = median_filter(labels, size=5)\n",
        "    # ============================================================\n",
        "\n",
        "    seconds_per_block = (STRIDE / fps) * (2**DEPTH)\n",
        "    segments = []\n",
        "    if len(labels) == 0: \n",
        "        return segments, labels\n",
        "\n",
        "    # Identify cluster transitions to define segment boundaries\n",
        "    current_label = labels[0]\n",
        "    start_idx = 0\n",
        "    for i, label in enumerate(labels):\n",
        "        if label != current_label:\n",
        "            segments.append((start_idx * seconds_per_block, i * seconds_per_block))\n",
        "            current_label = label\n",
        "            start_idx = i\n",
        "    segments.append((start_idx * seconds_per_block, len(labels) * seconds_per_block))\n",
        "    return segments, labels\n",
        "\n",
        "def compute_step_embeddings(original_features, segments, fps):\n",
        "    \"\"\"\n",
        "    Compute step-level embeddings by averaging features within segment boundaries.\n",
        "    \n",
        "    Args:\n",
        "        original_features (torch.Tensor): Original video features (T, feature_dim)\n",
        "        segments (list): Detected step boundaries as (start_time, end_time) tuples\n",
        "        fps (float): Frame rate for temporal scaling\n",
        "    \n",
        "    Returns:\n",
        "        np.ndarray: Step-level embeddings (num_steps, feature_dim)\n",
        "    \"\"\"\n",
        "    embeddings = []\n",
        "    feat_duration = STRIDE / fps\n",
        "    for start, end in segments:\n",
        "        start_idx = max(0, int(start / feat_duration))\n",
        "        end_idx = min(len(original_features), max(start_idx + 1, int(end / feat_duration)))\n",
        "        step_feat = original_features[start_idx:end_idx]\n",
        "        avg = torch.mean(step_feat, dim=0) if len(step_feat) > 0 else torch.zeros_like(original_features[0])\n",
        "        embeddings.append(avg.cpu().numpy())\n",
        "    return np.array(embeddings)\n",
        "\n",
        "# ==========================================\n",
        "# 3. MAIN PROCESSING LOOP\n",
        "# ==========================================\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Load video parameters from CSV\n",
        "video_params = load_params_from_dump(PARAMS_CSV)\n",
        "valid_ids_set = set(video_params.keys())\n",
        "\n",
        "# Load HiERO model\n",
        "print(\"Initializing HiERO model...\")\n",
        "model = build_hiero_model(CKPT_PATH)\n",
        "\n",
        "# Discover all feature files\n",
        "feat_files = glob.glob(os.path.join(INPUT_DIR_FEAT, '*.npy')) + glob.glob(os.path.join(INPUT_DIR_FEAT, '*.npz'))\n",
        "print(f\"Found {len(feat_files)} feature files. Processing matched videos...\")\n",
        "\n",
        "processed_count = 0\n",
        "skipped_count = 0\n",
        "\n",
        "for fpath in feat_files:\n",
        "    filename = os.path.basename(fpath)\n",
        "\n",
        "    # 1. Identify video ID from filename\n",
        "    video_id = find_matching_id(filename, valid_ids_set)\n",
        "\n",
        "    if not video_id:\n",
        "        continue\n",
        "\n",
        "    n_clusters_target = video_params[video_id]['n_clusters']\n",
        "    fps = video_params[video_id]['fps']\n",
        "\n",
        "    try:\n",
        "        # 2. Load original video features\n",
        "        if fpath.endswith('.npy'):\n",
        "            feats_np = np.load(fpath)\n",
        "        else:\n",
        "            # Handle .npz files with flexible key discovery\n",
        "            d = np.load(fpath)\n",
        "            feats_np = d['features'] if 'features' in d else d[list(d.keys())[0]]\n",
        "        feats_tensor = torch.from_numpy(feats_np).float()\n",
        "\n",
        "        # 3. Extract hierarchical features using HiERO\n",
        "        hiero_feats = extract_hiero_features(model, feats_tensor)\n",
        "\n",
        "        actual_clusters = min(n_clusters_target, len(hiero_feats))\n",
        "        if actual_clusters < 2:\n",
        "            print(f\"[SKIP] {video_id}: Too short or insufficient features for clustering.\")\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "\n",
        "        # 4. Perform clustering and temporal segmentation (with median filtering)\n",
        "        segments, labels = cluster_and_segment(hiero_feats, actual_clusters, fps)\n",
        "\n",
        "        # 5. Compute step-level embeddings by averaging features within segments\n",
        "        step_embeddings = compute_step_embeddings(feats_tensor, segments, fps)\n",
        "\n",
        "        # 6. Save results to disk\n",
        "        save_path = os.path.join(OUTPUT_DIR, f\"{video_id}_steps.npz\")\n",
        "        np.savez(save_path, segments=segments, embeddings=step_embeddings, labels=labels)\n",
        "\n",
        "        print(f\"[OK] {video_id} -> Saved (K={actual_clusters}, Segments={len(segments)})\")\n",
        "        processed_count += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] {video_id} ({filename}): {e}\")\n",
        "\n",
        "print(f\"\\n=== Processing Complete ===\")\n",
        "print(f\"Successfully processed: {processed_count} videos\")\n",
        "print(f\"Skipped: {skipped_count} videos\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
