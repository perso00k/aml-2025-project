{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Extension Substep 1: Recipe Step Localization using HiERO\n",
        "\n",
        "## Objective\n",
        "Segment individual steps of recipe videos using a zero-shot clustering-based approach (HiERO). \n",
        "\n",
        "## Workflow\n",
        "1. **Load pre-trained HiERO model**: Uses a graph neural network trained on hierarchical temporal modeling\n",
        "2. **Extract hierarchical features**: Computes multi-scale temporal representations from video features\n",
        "3. **Cluster and segment**: Uses spectral clustering with median filtering to identify temporal boundaries\n",
        "4. **Compute step embeddings**: Averages video features within detected step boundaries to obtain step-level representations\n",
        "\n",
        "## Output\n",
        "For each video: \n",
        "- A list of tuples `(start_time, end_time)` defining step boundaries\n",
        "- A sequence of step-level embeddings (averaged features for each detected step)\n",
        "- Cluster labels indicating temporal boundaries\n",
        "\n",
        "## Key Technologies\n",
        "- **HiERO**: Hierarchical temporal segmentation using graph neural networks\n",
        "- **Spectral Clustering**: Zero-shot clustering for temporal boundary detection\n",
        "- **Median Filtering**: Temporal smoothing to reduce noise in segmentation labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "P_-Fs_Gn1XJ0",
        "outputId": "597263c9-65da-4391-f592-287af9be3afa"
      },
      "outputs": [],
      "source": [
        "# @title 1. Setup Environment & Path Configuration\n",
        "\"\"\"\n",
        "Initialize the environment by mounting Google Drive and installing required dependencies.\n",
        "This setup ensures all necessary libraries and project code are accessible.\n",
        "\"\"\"\n",
        "import os\n",
        "import sys\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access project files and data\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- PATH CONFIGURATION ---\n",
        "# Define the project root directory where files are stored on Drive\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive/AML_Project/Extension/step_1_HiERO\"\n",
        "\n",
        "# Locate the HiERO code directory\n",
        "REPO_DIR = os.path.join(DRIVE_ROOT, \"HiERO\")\n",
        "\n",
        "# Verify the directory exists\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    raise FileNotFoundError(f\"Could not find code directory at: {REPO_DIR}. Please ensure the 'HiERO' folder has been correctly moved.\")\n",
        "\n",
        "print(f\"Working directory set to: {REPO_DIR}\")\n",
        "\n",
        "# Install Dependencies\n",
        "# Install required packages from requirements.txt with PyTorch GPU support\n",
        "%cd \"{REPO_DIR}\"\n",
        "!pip install -r requirements.txt -f https://data.pyg.org/whl/torch-2.4.0+cu124.html --extra-index-url https://download.pytorch.org/whl/\n",
        "!pip install hydra-core --upgrade\n",
        "\n",
        "# Add HiERO to Python path\n",
        "# Essential for Python to locate modules such as 'models', 'utils', etc.\n",
        "if REPO_DIR not in sys.path:\n",
        "    sys.path.append(REPO_DIR)\n",
        "\n",
        "print(\"Environment setup completed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byBM5e-f2sju"
      },
      "outputs": [],
      "source": [
        "# @title 2. Utility Functions (Model & Clustering)\n",
        "\"\"\"\n",
        "Core utility functions for HiERO model inference, hierarchical feature extraction,\n",
        "and temporal segmentation via spectral clustering.\n",
        "\"\"\"\n",
        "import torch\n",
        "import hydra\n",
        "import numpy as np\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from torch.nn import functional as F\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def build_hiero_model(ckpt_path, input_size=256, depth=2):\n",
        "    \"\"\"\n",
        "    Load the HiERO model from a checkpoint file.\n",
        "    \n",
        "    The model is instantiated with configuration saved in the checkpoint,\n",
        "    enabling hierarchical graph neural network processing.\n",
        "    \n",
        "    Args:\n",
        "        ckpt_path (str): Path to the model checkpoint file\n",
        "        input_size (int): Input feature dimension (default: 256 for EgoVLP)\n",
        "        depth (int): Hierarchical depth for feature extraction (default: 2)\n",
        "    \n",
        "    Returns:\n",
        "        torch.nn.Module: Loaded HiERO model in evaluation mode on the appropriate device\n",
        "    \n",
        "    Raises:\n",
        "        FileNotFoundError: If the checkpoint file does not exist\n",
        "    \"\"\"\n",
        "    if not os.path.exists(ckpt_path):\n",
        "        raise FileNotFoundError(f\"Checkpoint not found at: {ckpt_path}\")\n",
        "\n",
        "    print(f\"Loading model from {ckpt_path}...\")\n",
        "    weights = torch.load(ckpt_path, map_location=DEVICE)\n",
        "\n",
        "    # Instantiate model using saved configuration\n",
        "    model = hydra.utils.instantiate(\n",
        "        weights[\"config\"][\"model\"],\n",
        "        clustering_at_inference=True,\n",
        "        input_size=input_size,\n",
        "        _recursive_=False\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    model.load_state_dict(weights[\"model\"], strict=False)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def extract_hiero_features(model, features, depth=2):\n",
        "    \"\"\"\n",
        "    Perform inference to obtain hierarchical feature representations.\n",
        "    \n",
        "    Constructs a temporal graph from input features and extracts\n",
        "    multi-scale temporal representations at the specified hierarchical depth.\n",
        "    \n",
        "    Args:\n",
        "        model: HiERO model instance\n",
        "        features (torch.Tensor): Input video features of shape (T, feature_dim)\n",
        "        depth (int): Hierarchical level for feature extraction (default: 2)\n",
        "    \n",
        "    Returns:\n",
        "        torch.Tensor: Hierarchical features at the specified depth\n",
        "    \"\"\"\n",
        "    features = features.to(DEVICE)\n",
        "\n",
        "    # Build dummy temporal graph structure\n",
        "    pos = torch.arange(0, features.shape[0], device=DEVICE).float()\n",
        "    indices = torch.arange(0, features.shape[0], device=DEVICE)\n",
        "    batch = torch.zeros_like(indices, dtype=torch.long)\n",
        "    mask = torch.ones_like(indices, dtype=torch.bool)\n",
        "\n",
        "    data = Data(x=features.unsqueeze(1), pos=pos, indices=indices, batch=batch, mask=mask)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        graphs = model(data)\n",
        "        out_features = graphs.x[graphs.depth == depth]\n",
        "\n",
        "    return out_features\n",
        "\n",
        "def clusterize_segments(features, n_clusters=5, temp=0.05):\n",
        "    \"\"\"\n",
        "    Cluster features using spectral clustering to identify temporal segments.\n",
        "    \n",
        "    Computes similarity affinity matrix using cosine distance with temperature scaling,\n",
        "    then applies spectral clustering to group temporally similar features.\n",
        "    \n",
        "    Args:\n",
        "        features (torch.Tensor): Features to cluster of shape (N, feature_dim)\n",
        "        n_clusters (int): Target number of clusters (default: 5)\n",
        "        temp (float): Temperature parameter for affinity computation (default: 0.05)\n",
        "    \n",
        "    Returns:\n",
        "        np.ndarray: Cluster labels for each temporal position\n",
        "    \"\"\"\n",
        "    features = F.normalize(features, p=2, dim=-1)\n",
        "    affinity = torch.exp((features @ features.T) / temp).cpu().numpy()\n",
        "    sc = SpectralClustering(n_clusters=n_clusters, affinity=\"precomputed\", assign_labels='kmeans')\n",
        "    labels = sc.fit_predict(affinity)\n",
        "    return labels\n",
        "\n",
        "def get_segments_timestamps(labels, fps, stride=16, depth=2):\n",
        "    \"\"\"\n",
        "    Convert cluster labels to temporal segment boundaries.\n",
        "    \n",
        "    Identifies transitions in cluster labels and converts them to\n",
        "    (start_time, end_time) tuples based on video FPS and feature stride.\n",
        "    \n",
        "    Args:\n",
        "        labels (np.ndarray): Cluster labels for each position\n",
        "        fps (float): Frames per second of the original video\n",
        "        stride (int): Frame stride used in feature extraction (default: 16)\n",
        "        depth (int): Hierarchical depth (affects temporal scale) (default: 2)\n",
        "    \n",
        "    Returns:\n",
        "        list: List of (start_time, end_time) tuples in seconds\n",
        "    \"\"\"\n",
        "    seconds_per_block = (stride / fps) * (2**depth)\n",
        "    segments = []\n",
        "    if len(labels) == 0:\n",
        "        return segments\n",
        "\n",
        "    current_label = labels[0]\n",
        "    start_idx = 0\n",
        "\n",
        "    for i, label in enumerate(labels):\n",
        "        if label != current_label:\n",
        "            end_idx = i\n",
        "            start_time = start_idx * seconds_per_block\n",
        "            end_time = end_idx * seconds_per_block\n",
        "            segments.append((start_time, end_time))\n",
        "            current_label = label\n",
        "            start_idx = i\n",
        "\n",
        "    # Append final segment\n",
        "    start_time = start_idx * seconds_per_block\n",
        "    end_time = len(labels) * seconds_per_block\n",
        "    segments.append((start_time, end_time))\n",
        "\n",
        "    return segments\n",
        "\n",
        "def average_features_within_steps(original_features, segments, fps, stride=16):\n",
        "    \"\"\"\n",
        "    Compute step-level embeddings by averaging features within temporal boundaries.\n",
        "    \n",
        "    For each detected step (segment), pools the original video features that fall\n",
        "    within the segment boundaries, creating a single representative embedding.\n",
        "    \n",
        "    Args:\n",
        "        original_features (torch.Tensor): Original video features (T, feature_dim)\n",
        "        segments (list): List of (start_time, end_time) tuples\n",
        "        fps (float): Frames per second of the video\n",
        "        stride (int): Frame stride used in feature extraction (default: 16)\n",
        "    \n",
        "    Returns:\n",
        "        np.ndarray: Step-level embeddings, shape (num_segments, feature_dim)\n",
        "    \"\"\"\n",
        "    step_embeddings = []\n",
        "    feat_duration = stride / fps\n",
        "\n",
        "    for start, end in segments:\n",
        "        start_idx = int(start / feat_duration)\n",
        "        end_idx = int(end / feat_duration)\n",
        "        start_idx = max(0, start_idx)\n",
        "        end_idx = min(len(original_features), max(start_idx + 1, end_idx))\n",
        "\n",
        "        step_feat = original_features[start_idx:end_idx]\n",
        "\n",
        "        if len(step_feat) > 0:\n",
        "            avg_feat = torch.mean(step_feat, dim=0)\n",
        "        else:\n",
        "            avg_feat = torch.zeros_like(original_features[0])\n",
        "\n",
        "        step_embeddings.append(avg_feat.cpu().numpy())\n",
        "\n",
        "    return np.array(step_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rZHCQWa9pqy",
        "outputId": "8eab2495-0fe4-468d-db45-681bf131e6be"
      },
      "outputs": [],
      "source": [
        "# @title 1. Load Metadata & Configure Parameters\n",
        "\"\"\"\n",
        "Load video metadata from CSV files and build a parameter dictionary\n",
        "for HiERO processing. Maps each video to the number of recipe steps\n",
        "and frame rate.\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "\n",
        "# --- PATH CONFIGURATION ---\n",
        "# Ensure these paths correctly point to your annotations and task graph files\n",
        "CSV_STEPS_PATH = '/content/drive/MyDrive/AML_Project/annotations-main/annotation_csv/recording_id_step_idx.csv'\n",
        "CSV_NAMES_PATH = '/content/drive/MyDrive/AML_Project/annotations-main/annotation_csv/average_segment_length.csv'\n",
        "CSV_TASK_GRAPHS_PATH = '/content/drive/MyDrive/AML_Project/annotations-main/task_graphs'\n",
        "\n",
        "def load_video_parameters(steps_csv, names_csv):\n",
        "    \"\"\"\n",
        "    Create a dictionary mapping video IDs to their processing parameters.\n",
        "    \n",
        "    For each video, determines:\n",
        "    - n_clusters: The number of unique recipe steps in the task\n",
        "    - activity_name: The name of the recipe\n",
        "    - fps: Frame rate of the video (for temporal scaling)\n",
        "    \n",
        "    This information is loaded from CSV files and task graph JSON files,\n",
        "    providing semantic structure for the clustering process.\n",
        "    \n",
        "    Args:\n",
        "        steps_csv (str): Path to CSV file with step indices for each recording\n",
        "        names_csv (str): Path to CSV file with recipe names and metadata\n",
        "    \n",
        "    Returns:\n",
        "        dict: Mapping from video_id to {n_clusters, activity_name, activity_id, fps}\n",
        "    \n",
        "    Raises:\n",
        "        FileNotFoundError: If CSV files or task graph files cannot be found\n",
        "    \"\"\"\n",
        "    print(\"Loading metadata...\")\n",
        "\n",
        "    # Load CSV files\n",
        "    df_steps = pd.read_csv(steps_csv)\n",
        "    df_names = pd.read_csv(names_csv)\n",
        "\n",
        "    # Ensure IDs are strings for consistent matching\n",
        "    df_steps['activity_id'] = df_steps['activity_id'].astype(str)\n",
        "    df_names['activity_id'] = df_names['activity_id'].astype(str)\n",
        "\n",
        "    # Merge dataframes on activity_id to associate recipe names with recordings\n",
        "    df_merged = pd.merge(df_steps, df_names, on='activity_id', how='left')\n",
        "\n",
        "    video_params = {}\n",
        "\n",
        "    for _, row in df_merged.iterrows():\n",
        "        video_id = str(row['recording_id'])\n",
        "\n",
        "        # --- COMPUTE N_CLUSTERS ---\n",
        "        # Strategy: Number of clusters equals the number of unique steps in the recipe.\n",
        "        # Example: step sequence \"3,1,4,1,3\" -> Unique steps: {1,3,4} -> n_clusters=3\n",
        "        # HiERO will group semantically similar features; temporal boundaries separate repetitions.\n",
        "        try:\n",
        "            step_list = [int(s) for s in str(row['step_indices']).split(',')]\n",
        "            n_unique_steps = len(set(step_list))\n",
        "            s = row['activity_name'].replace(' ', '').lower()\n",
        "            with open(f\"{CSV_TASK_GRAPHS_PATH}/{s}.json\", \"r\") as f:\n",
        "              data = json.load(f)\n",
        "            num_steps = len(data[\"steps\"])\n",
        "            # Safety: minimum 2 clusters required for clustering algorithm\n",
        "            #n_clusters = max(2, n_unique_steps)\n",
        "        except:\n",
        "            print(f\"Warning: Could not parse steps for {video_id}, using default n_clusters=7\")\n",
        "            num_steps = 7\n",
        "\n",
        "        # --- FRAME RATE (FPS) ---\n",
        "        # CSV files do not contain FPS information directly.\n",
        "        # Default value of 30.0 is used; main processing loop refines this from video metadata.\n",
        "        fps = 30.0\n",
        "\n",
        "        video_params[video_id] = {\n",
        "            'n_clusters': num_steps,\n",
        "            'activity_name': row['activity_name'],\n",
        "            'activity_id': row['activity_id'],\n",
        "            'fps': fps\n",
        "        }\n",
        "\n",
        "    print(f\"Loaded parameters for {len(video_params)} videos.\")\n",
        "    return video_params\n",
        "\n",
        "# Execute metadata loading\n",
        "try:\n",
        "    VIDEO_PARAMS = load_video_parameters(CSV_STEPS_PATH, CSV_NAMES_PATH)\n",
        "\n",
        "    # Print sample for verification\n",
        "    sample_id = list(VIDEO_PARAMS.keys())[0]\n",
        "    print(f\"\\nSample video {sample_id}: {VIDEO_PARAMS[sample_id]}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading CSV files: {e}\")\n",
        "    # Fallback to empty dictionary to prevent blocking\n",
        "    VIDEO_PARAMS = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QngYmZZ8MnSv",
        "outputId": "b92e4acb-92a1-4e34-a491-fe7031051433"
      },
      "outputs": [],
      "source": [
        "# @title HiERO Step Localization (Robust Matching Version + Median Filter)\n",
        "\"\"\"\n",
        "Main processing pipeline for recipe step localization using HiERO.\n",
        "\n",
        "Workflow:\n",
        "1. Load hierarchical features from HiERO model inference\n",
        "2. Apply spectral clustering to identify temporal clusters\n",
        "3. Filter noise using median filtering on cluster labels\n",
        "4. Convert clusters to temporal step boundaries\n",
        "5. Compute step-level embeddings by averaging original features\n",
        "6. Save results (segments, embeddings, labels) per video\n",
        "\"\"\"\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import hydra\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from torch.nn import functional as F\n",
        "from scipy.ndimage import median_filter  # Temporal smoothing filter\n",
        "\n",
        "# ==========================================\n",
        "# 1. CONFIGURATION\n",
        "# ==========================================\n",
        "\n",
        "# Input/Output paths\n",
        "INPUT_DIR_FEAT = '/content/drive/MyDrive/AML_Project/3_EgoVLP/features'\n",
        "OUTPUT_DIR     = '/content/drive/MyDrive/AML_Project/Extension/step_1_HiERO/steps_v4'\n",
        "PARAMS_CSV     = '/content/drive/MyDrive/AML_Project/Extension/step_1_HiERO/video_params_dump.csv'\n",
        "CKPT_PATH      = '/content/drive/MyDrive/AML_Project/Extension/step_1_HiERO/HiERO/checkpoints/hiero_egovlp.pth'\n",
        "\n",
        "# Fixed HiERO parameters\n",
        "STRIDE = 16  # Frame stride in feature extraction (affects temporal resolution)\n",
        "DEPTH = 2    # Hierarchical depth for feature extraction\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ==========================================\n",
        "# 2. CORE FUNCTIONS\n",
        "# ==========================================\n",
        "\n",
        "def load_params_from_dump(csv_path):\n",
        "    \"\"\"\n",
        "    Load video processing parameters (n_clusters and fps) from CSV dump file.\n",
        "    \n",
        "    Args:\n",
        "        csv_path (str): Path to the parameters CSV file\n",
        "    \n",
        "    Returns:\n",
        "        dict: Mapping from video_id to {n_clusters, fps}\n",
        "    \n",
        "    Raises:\n",
        "        FileNotFoundError: If CSV file does not exist\n",
        "    \"\"\"\n",
        "    print(f\"Loading parameters from: {csv_path}\")\n",
        "    if not os.path.exists(csv_path):\n",
        "        raise FileNotFoundError(f\"CSV not found: {csv_path}\")\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Ensure video_id is string and whitespace-trimmed\n",
        "    df['video_id'] = df['video_id'].astype(str).str.strip()\n",
        "\n",
        "    # Build fast lookup dictionary: id -> {n_clusters, fps}\n",
        "    params_map = {}\n",
        "    for _, row in df.iterrows():\n",
        "        # Safe fps fallback to 30.0 if missing\n",
        "        fps_val = float(row['fps']) if 'fps' in row and not pd.isna(row['fps']) else 30.0\n",
        "\n",
        "        params_map[row['video_id']] = {\n",
        "            'n_clusters': int(row['n_clusters']),\n",
        "            'fps': fps_val\n",
        "        }\n",
        "    return params_map\n",
        "\n",
        "def find_matching_id(filename, valid_ids):\n",
        "    \"\"\"\n",
        "    Find which valid video ID is contained in the feature filename.\n",
        "    \n",
        "    Attempts multiple matching strategies (exact match, prefix match)\n",
        "    to robustly identify the video ID from filename.\n",
        "    \n",
        "    Args:\n",
        "        filename (str): Feature file name\n",
        "        valid_ids (set): Set of valid video IDs from parameters\n",
        "    \n",
        "    Returns:\n",
        "        str or None: Matched video ID, or None if no match found\n",
        "    \"\"\"\n",
        "    clean_name = os.path.splitext(filename)[0]\n",
        "    if clean_name in valid_ids:\n",
        "        return clean_name\n",
        "\n",
        "    # Try prefix matching\n",
        "    candidates = []\n",
        "    for vid in valid_ids:\n",
        "        if filename.startswith(vid + \"_\") or filename.startswith(vid + \".\"):\n",
        "            candidates.append(vid)\n",
        "\n",
        "    if candidates:\n",
        "        return max(candidates, key=len)\n",
        "\n",
        "    return None\n",
        "\n",
        "def build_hiero_model(ckpt_path):\n",
        "    \"\"\"\n",
        "    Load HiERO model from checkpoint (wrapper for utility function).\n",
        "    \n",
        "    Args:\n",
        "        ckpt_path (str): Path to model checkpoint\n",
        "    \n",
        "    Returns:\n",
        "        torch.nn.Module: Loaded model in evaluation mode\n",
        "    \"\"\"\n",
        "    weights = torch.load(ckpt_path, map_location=DEVICE)\n",
        "    model = hydra.utils.instantiate(weights[\"config\"][\"model\"], clustering_at_inference=True, input_size=256, _recursive_=False).to(DEVICE)\n",
        "    model.load_state_dict(weights[\"model\"], strict=False)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def extract_hiero_features(model, features):\n",
        "    \"\"\"\n",
        "    Extract hierarchical features using HiERO model (wrapper for utility function).\n",
        "    \n",
        "    Args:\n",
        "        model: HiERO model instance\n",
        "        features (torch.Tensor): Input features (T, feature_dim)\n",
        "    \n",
        "    Returns:\n",
        "        torch.Tensor: Hierarchical features at depth=DEPTH\n",
        "    \"\"\"\n",
        "    features = features.to(DEVICE)\n",
        "    pos = torch.arange(0, features.shape[0], device=DEVICE).float()\n",
        "    indices = torch.arange(0, features.shape[0], device=DEVICE)\n",
        "    batch = torch.zeros_like(indices, dtype=torch.long)\n",
        "    mask = torch.ones_like(indices, dtype=torch.bool)\n",
        "    data = Data(x=features.unsqueeze(1), pos=pos, indices=indices, batch=batch, mask=mask)\n",
        "    with torch.no_grad():\n",
        "        graphs = model(data)\n",
        "        return graphs.x[graphs.depth == DEPTH]\n",
        "\n",
        "def cluster_and_segment(features, n_clusters, fps):\n",
        "    \"\"\"\n",
        "    Perform spectral clustering and convert cluster labels to temporal segments.\n",
        "    \n",
        "    Applies median filtering to the cluster labels to reduce noise and ensure\n",
        "    temporal stability of segment boundaries.\n",
        "    \n",
        "    Args:\n",
        "        features (torch.Tensor): Hierarchical features to cluster\n",
        "        n_clusters (int): Target number of recipe steps\n",
        "        fps (float): Frame rate for temporal scaling\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (segments, labels) where:\n",
        "            - segments: List of (start_time, end_time) tuples\n",
        "            - labels: Filtered cluster labels array\n",
        "    \"\"\"\n",
        "    features_norm = F.normalize(features, p=2, dim=-1)\n",
        "    affinity = torch.exp((features_norm @ features_norm.T) / 0.05).cpu().numpy()\n",
        "    assign_labels = 'discretize' if n_clusters < 3 else 'kmeans'\n",
        "\n",
        "    # 1. Perform spectral clustering\n",
        "    sc = SpectralClustering(n_clusters=n_clusters, affinity=\"precomputed\", assign_labels=assign_labels, random_state=42)\n",
        "    labels = sc.fit_predict(affinity)\n",
        "\n",
        "    # ============================================================\n",
        "    # MEDIAN FILTERING FOR TEMPORAL SMOOTHING\n",
        "    # ============================================================\n",
        "    # Kernel size=5 reduces noise and prevents overly fragmented segments.\n",
        "    # Ensures temporal stability of cluster labels before computing boundaries.\n",
        "    if len(labels) > 0:\n",
        "        labels = median_filter(labels, size=5)\n",
        "    # ============================================================\n",
        "\n",
        "    seconds_per_block = (STRIDE / fps) * (2**DEPTH)\n",
        "    segments = []\n",
        "    if len(labels) == 0: \n",
        "        return segments, labels\n",
        "\n",
        "    # Identify cluster transitions to define segment boundaries\n",
        "    current_label = labels[0]\n",
        "    start_idx = 0\n",
        "    for i, label in enumerate(labels):\n",
        "        if label != current_label:\n",
        "            segments.append((start_idx * seconds_per_block, i * seconds_per_block))\n",
        "            current_label = label\n",
        "            start_idx = i\n",
        "    segments.append((start_idx * seconds_per_block, len(labels) * seconds_per_block))\n",
        "    return segments, labels\n",
        "\n",
        "def compute_step_embeddings(original_features, segments, fps):\n",
        "    \"\"\"\n",
        "    Compute step-level embeddings by averaging features within segment boundaries.\n",
        "    \n",
        "    Args:\n",
        "        original_features (torch.Tensor): Original video features (T, feature_dim)\n",
        "        segments (list): Detected step boundaries as (start_time, end_time) tuples\n",
        "        fps (float): Frame rate for temporal scaling\n",
        "    \n",
        "    Returns:\n",
        "        np.ndarray: Step-level embeddings (num_steps, feature_dim)\n",
        "    \"\"\"\n",
        "    embeddings = []\n",
        "    feat_duration = STRIDE / fps\n",
        "    for start, end in segments:\n",
        "        start_idx = max(0, int(start / feat_duration))\n",
        "        end_idx = min(len(original_features), max(start_idx + 1, int(end / feat_duration)))\n",
        "        step_feat = original_features[start_idx:end_idx]\n",
        "        avg = torch.mean(step_feat, dim=0) if len(step_feat) > 0 else torch.zeros_like(original_features[0])\n",
        "        embeddings.append(avg.cpu().numpy())\n",
        "    return np.array(embeddings)\n",
        "\n",
        "# ==========================================\n",
        "# 3. MAIN PROCESSING LOOP\n",
        "# ==========================================\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Load video parameters from CSV\n",
        "video_params = load_params_from_dump(PARAMS_CSV)\n",
        "valid_ids_set = set(video_params.keys())\n",
        "\n",
        "# Load HiERO model\n",
        "print(\"Initializing HiERO model...\")\n",
        "model = build_hiero_model(CKPT_PATH)\n",
        "\n",
        "# Discover all feature files\n",
        "feat_files = glob.glob(os.path.join(INPUT_DIR_FEAT, '*.npy')) + glob.glob(os.path.join(INPUT_DIR_FEAT, '*.npz'))\n",
        "print(f\"Found {len(feat_files)} feature files. Processing matched videos...\")\n",
        "\n",
        "processed_count = 0\n",
        "skipped_count = 0\n",
        "\n",
        "for fpath in feat_files:\n",
        "    filename = os.path.basename(fpath)\n",
        "\n",
        "    # 1. Identify video ID from filename\n",
        "    video_id = find_matching_id(filename, valid_ids_set)\n",
        "\n",
        "    if not video_id:\n",
        "        continue\n",
        "\n",
        "    n_clusters_target = video_params[video_id]['n_clusters']\n",
        "    fps = video_params[video_id]['fps']\n",
        "\n",
        "    try:\n",
        "        # 2. Load original video features\n",
        "        if fpath.endswith('.npy'):\n",
        "            feats_np = np.load(fpath)\n",
        "        else:\n",
        "            # Handle .npz files with flexible key discovery\n",
        "            d = np.load(fpath)\n",
        "            feats_np = d['features'] if 'features' in d else d[list(d.keys())[0]]\n",
        "        feats_tensor = torch.from_numpy(feats_np).float()\n",
        "\n",
        "        # 3. Extract hierarchical features using HiERO\n",
        "        hiero_feats = extract_hiero_features(model, feats_tensor)\n",
        "\n",
        "        actual_clusters = min(n_clusters_target, len(hiero_feats))\n",
        "        if actual_clusters < 2:\n",
        "            print(f\"[SKIP] {video_id}: Too short or insufficient features for clustering.\")\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "\n",
        "        # 4. Perform clustering and temporal segmentation (with median filtering)\n",
        "        segments, labels = cluster_and_segment(hiero_feats, actual_clusters, fps)\n",
        "\n",
        "        # 5. Compute step-level embeddings by averaging features within segments\n",
        "        step_embeddings = compute_step_embeddings(feats_tensor, segments, fps)\n",
        "\n",
        "        # 6. Save results to disk\n",
        "        save_path = os.path.join(OUTPUT_DIR, f\"{video_id}_steps.npz\")\n",
        "        np.savez(save_path, segments=segments, embeddings=step_embeddings, labels=labels)\n",
        "\n",
        "        print(f\"[OK] {video_id} -> Saved (K={actual_clusters}, Segments={len(segments)})\")\n",
        "        processed_count += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] {video_id} ({filename}): {e}\")\n",
        "\n",
        "print(f\"\\n=== Processing Complete ===\")\n",
        "print(f\"Successfully processed: {processed_count} videos\")\n",
        "print(f\"Skipped: {skipped_count} videos\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
