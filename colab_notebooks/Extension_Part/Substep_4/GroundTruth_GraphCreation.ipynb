{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import os\n","import json\n","import torch\n","import numpy as np\n","from tqdm import tqdm\n","from google.colab import drive\n","\n","if not os.path.exists('/content/drive'):\n","    drive.mount('/content/drive')\n","else:\n","    print(\"Google Drive already mounted\")"],"metadata":{"id":"XhdYf3fbXhij"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- PATH CONFIGURATION ---\n","GT_ANNOTATIONS_PATH = '/content/drive/MyDrive/AML_Project/annotations-main/annotation_json/complete_step_annotations.json'\n","ERROR_ANNOTATIONS_PATH = '/content/drive/MyDrive/AML_Project/annotations-main/annotation_json/error_annotations.json'\n","\n","VIDEO_FEATURES_DIR = '/content/drive/MyDrive/AML_Project/3_EgoVLP/features'\n","TEXT_FEATURES_DIR = '/content/drive/MyDrive/AML_Project/Extension/step_3_task_graph/text_features_egovlp'\n","TASK_GRAPHS_DIR = '/content/drive/MyDrive/AML_Project/annotations-main/task_graphs'\n","\n","# Output\n","OUTPUT_DIR = '/content/drive/MyDrive/AML_Project/Extension/step_4_gnn/gnn_ready_data_groundtruth'\n","os.makedirs(OUTPUT_DIR, exist_ok=True)"],"metadata":{"id":"1VBPTjN8X1eF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- METADATA LOADING ---\n","print(\"Loading Ground Truth Annotations...\")\n","with open(GT_ANNOTATIONS_PATH, 'r') as f:\n","    gt_data = json.load(f)\n","\n","with open(ERROR_ANNOTATIONS_PATH, 'r') as f:\n","    error_list = json.load(f)\n","    error_map = {item['recording_id']: (1 if item['is_error'] else 0) for item in error_list}"],"metadata":{"id":"eP8mpHbJX3_y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- HELPER FUNCTIONS ---\n","def get_video_features(recording_id, start_time, end_time):\n","    \"\"\"\n","    Loads the video .npy file and extracts the average features in the time segment.\n","    Naming convention: {recording_id}_360p_224.mp4_1s_1s.npy\n","    \"\"\"\n","    filename = f\"{recording_id}_360p_224.mp4_1s_1s.npy\"\n","    path_npy = os.path.join(VIDEO_FEATURES_DIR, filename)\n","    path_npz = path_npy.replace('.npy', '.npz') # Fallback\n","\n","    features = None\n","\n","    # Loading attempt\n","    if os.path.exists(path_npy):\n","        try: features = np.load(path_npy)\n","        except: pass\n","    elif os.path.exists(path_npz):\n","        try:\n","            data = np.load(path_npz)\n","            features = data['arr_0'] if 'arr_0' in data else data[data.files[0]]\n","        except: pass\n","\n","    if features is None:\n","        return torch.zeros(256) # Fallback if file not found\n","\n","    # Temporal slicing (1 feature per second)\n","    fps = 1\n","    total_frames = features.shape[0]\n","    start_idx = max(0, int(np.floor(start_time * fps)))\n","    end_idx = min(total_frames, int(np.ceil(end_time * fps)))\n","\n","    if start_idx < end_idx:\n","        segment = features[start_idx:end_idx]\n","        return torch.tensor(segment).float().mean(dim=0)\n","    else:\n","        # If segment is a single point or out of range, we take the closest frame\n","        safe_idx = min(start_idx, total_frames - 1)\n","        return torch.tensor(features[safe_idx]).float()\n","\n","def load_text_features(recipe_name):\n","    \"\"\"\n","    Loads text features from the .pt file corresponding to the recipe.\n","    Expected filename: blenderbananapancakes.pt (all lowercase, no spaces)\n","    \"\"\"\n","    safe_name = recipe_name.lower().replace(\" \", \"\").replace(\"-\", \"\") + \".pt\"\n","    path = os.path.join(TEXT_FEATURES_DIR, safe_name)\n","\n","    if os.path.exists(path):\n","        try:\n","            # The .pt file saved in the previous substep is a dictionary with key 'text_features'\n","            data = torch.load(path)\n","            if isinstance(data, dict) and 'text_features' in data:\n","                return data['text_features'].float()\n","            elif isinstance(data, torch.Tensor):\n","                return data.float()\n","        except Exception as e:\n","            print(f\"Text loading error for {safe_name}: {e}\")\n","\n","    return None # Returns None if it fails\n","\n","def get_canonical_graph(activity_name):\n","    \"\"\"Retrieves the graph structure (nodes and edges) and sorted IDs.\"\"\"\n","    safe_name = activity_name.lower().replace(\" \", \"\").replace(\"-\", \"\") + \".json\"\n","    json_path = os.path.join(TASK_GRAPHS_DIR, safe_name)\n","\n","    # File search fallback\n","    if not os.path.exists(json_path) and os.path.exists(TASK_GRAPHS_DIR):\n","        for f in os.listdir(TASK_GRAPHS_DIR):\n","            if f.lower().replace(\" \", \"\").replace(\"-\", \"\") == safe_name:\n","                json_path = os.path.join(TASK_GRAPHS_DIR, f)\n","                break\n","\n","    if os.path.exists(json_path):\n","        with open(json_path, 'r') as f:\n","            data = json.load(f)\n","\n","        steps = data.get('steps', {})\n","        sorted_ids = sorted([int(k) for k in steps.keys()]) # Important: numerical sorting\n","        id_to_idx = {str(sid): i for i, sid in enumerate(sorted_ids)}\n","\n","        edges = []\n","        for key in ['edges', 'adjacency', 'successors']:\n","            if key in data:\n","                struct = data[key]\n","                if isinstance(struct, list): edges = struct\n","                elif isinstance(struct, dict):\n","                    for u, neighbors in struct.items():\n","                        for v in neighbors: edges.append([u, v])\n","                break\n","\n","        final_edges = []\n","        for u, v in edges:\n","            if str(u) in id_to_idx and str(v) in id_to_idx:\n","                final_edges.append([id_to_idx[str(u)], id_to_idx[str(v)]])\n","\n","        return id_to_idx, final_edges\n","    return {}, []"],"metadata":{"id":"4Cx-_CzWX70C"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vI1I8Lj-XU4k"},"outputs":[],"source":["# --- 4. GENERATION LOOP ---\n","print(f\"Starting Ground Truth dataset creation for {len(gt_data)} videos...\")\n","count = 0\n","\n","for recording_id, data in tqdm(gt_data.items()):\n","    activity_name = data['activity_name']\n","\n","    # 1. Graph Structure\n","    step_id_to_idx, edges = get_canonical_graph(activity_name)\n","    if not step_id_to_idx: continue\n","\n","    num_nodes = len(step_id_to_idx)\n","\n","    # 2. Loading Textual Features (from .pt file)\n","    # Features in the .pt file are already sorted by increasing step ID (like step_id_to_idx)\n","    text_tensor = load_text_features(activity_name)\n","\n","    if text_tensor is not None and text_tensor.shape[0] == num_nodes:\n","        x_text = text_tensor\n","    else:\n","        # Fallback if we don't have the text or dimensions do not match\n","        # (This should not happen if substep 3 was correct)\n","        x_text = torch.zeros((num_nodes, 256))\n","\n","    # 3. Loading Video Features (from .npy files + GT time)\n","    x_video = torch.zeros((num_nodes, 256))\n","\n","    for step in data['steps']:\n","        step_id = str(step['step_id'])\n","        if step_id in step_id_to_idx:\n","            idx = step_id_to_idx[step_id]\n","            start, end = step['start_time'], step['end_time']\n","\n","            # If the step was not performed (-1.0) we skip (it stays at zero)\n","            if start < 0: continue\n","\n","            vid_feat = get_video_features(recording_id, start, end)\n","\n","            if x_video[idx].abs().sum() == 0:\n","                x_video[idx] = vid_feat\n","            else:\n","                x_video[idx] = (x_video[idx] + vid_feat) / 2\n","\n","    # 4. Saving\n","    is_error = error_map.get(recording_id, 0)\n","    edge_index = torch.tensor(edges).t().long() if edges else torch.empty((2, 0)).long()\n","\n","    payload = {\n","        \"vid_id\": recording_id,\n","        \"recipe\": activity_name,\n","        \"x_text\": x_text.cpu(),\n","        \"x_video\": x_video.cpu(),\n","        \"edge_index\": edge_index,\n","        \"y\": torch.tensor(is_error, dtype=torch.float)\n","    }\n","\n","    torch.save(payload, os.path.join(OUTPUT_DIR, f\"gnn_ready_gt_{recording_id}.pt\"))\n","    count += 1\n","\n","print(f\"\\nCompleted! {count} files saved in: {OUTPUT_DIR}\")"]}]}