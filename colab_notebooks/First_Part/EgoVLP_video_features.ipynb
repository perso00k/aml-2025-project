{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "J5JY6CpEaP_b",
        "outputId": "5760fb2d-019f-40a2-8cf8-6f779de23cc9"
      },
      "outputs": [],
      "source": [
        "# Setup: Mount Google Drive and install required dependencies\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install required packages for EgoVLP\n",
        "!pip install decord\n",
        "!pip install transformers ftfy regex tqdm\n",
        "!pip install timm==0.4.12\n",
        "!pip install av\n",
        "!pip install ffmpeg-python\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/AML_Project/3_EgoVLP')\n",
        "\n",
        "# Clone EgoVLP repository if not already present\n",
        "if not os.path.exists('EgoVLP-main'):\n",
        "    !git clone https://github.com/showlab/EgoVLP.git EgoVLP-main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 3: Extending Baselines to New Feature Extraction Backbone - EgoVLP\n",
        "\n",
        "## Overview\n",
        "\n",
        "This section extends the CaptainCook4D baselines by integrating a new feature extraction backbone: **EgoVLP** (Egocentric Vision-Language Pre-training).\n",
        "\n",
        "**Objectives:**\n",
        "- Extract video features using the EgoVLP backbone\n",
        "- Adapt the CaptainCook4D feature extraction pipeline for EgoVLP\n",
        "- Generate 256-dimensional feature vectors at 1-second intervals\n",
        "- Enable comparison of LSTM baseline with different feature modalities\n",
        "\n",
        "**EgoVLP Architecture:**\n",
        "- **Video Encoder**: SpaceTimeTransformer (16 frames sampled per second)\n",
        "- **Text Encoder**: BERT-base-uncased\n",
        "- **Feature Dimension**: 256\n",
        "- **Training**: Pre-trained on egocentric video-text pairs\n",
        "\n",
        "**Key Tasks:**\n",
        "1. Download and setup EgoVLP model and dependencies\n",
        "2. Extract video features per second from cooking videos\n",
        "3. Save features as compressed numpy arrays\n",
        "4. Enable reproducibility and efficient batch processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409,
          "referenced_widgets": [
            "c8f03be4fac6499f81d12c0428da867b",
            "a9de632ca6f34fcfa25223c5bce05f5d",
            "11b48ad8713743c38f5a9065e80c04c1",
            "6eb8aa825295441aae803f362621c614",
            "2843b16408144187a16564cb0782b9c2",
            "0fcdbaccdfe245e88075a92bb4016813",
            "ef6588151fa54fe49ee6705f4e49dae7",
            "5b9f6ee0c733413ba440054f5729fc1b",
            "dea737b89b974ed89558614690f09643",
            "a543ef167d994091b5a30ce573ad0e4a",
            "e455de49065c479d9956123a7c2d15b8",
            "c5e45f644d5c4eca8893fc4c3daf5bef",
            "3f4c3682b6fc4dc3a3844eed5833082f",
            "1caccc5c629b4727bc50eb69203ee865",
            "f75ff9cd770b444f9591ed9bafabb725",
            "46117b76f40b48e28c117018832742ac",
            "2e14faabfa8440e487f07c02a0b6e8f0",
            "5399c3887c9442afa28ccd42e100a6a9",
            "4614ec70d5954d59baa7e63ac15901a9",
            "6b2b6bf7f0694aa89c2fa0f23190c9d2",
            "27b5f680ea6441d29a4feba3fd7581dd",
            "3b22e3e9c0f646ed810c0adf9ed172db"
          ]
        },
        "id": "LEXg6BiSam9z",
        "outputId": "d5bf4b11-9624-4489-fbd5-6b20e1cbbc8c"
      },
      "outputs": [],
      "source": [
        "# Part 3: EgoVLP Feature Extraction Pipeline for CaptainCook4D\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import glob\n",
        "import shutil\n",
        "import urllib.request\n",
        "from tqdm import tqdm\n",
        "from decord import VideoReader, cpu\n",
        "from torchvision.transforms import Compose, Lambda\n",
        "from torchvision.transforms._transforms_video import CenterCropVideo, NormalizeVideo\n",
        "\n",
        "# Path configuration\n",
        "DRIVE_ROOT = '/content/drive/MyDrive/AML_Project/3_EgoVLP'\n",
        "REPO_PATH = os.path.join(DRIVE_ROOT, 'EgoVLP-main')\n",
        "CHECKPOINT_PATH = os.path.join(DRIVE_ROOT, 'checkpoints/egovlp.pth')\n",
        "VIDEO_DIR = os.path.join(DRIVE_ROOT, 'videos')\n",
        "FEATURES_DIR = os.path.join(DRIVE_ROOT, 'features')\n",
        "TEMP_WORK_DIR = '/content/temp_video_processing'\n",
        "\n",
        "# --- Setup and Utilities ---\n",
        "def ensure_pretrained_backbone():\n",
        "    \"\"\"\n",
        "    Downloads ViT backbone weights (ImageNet) if not present.\n",
        "    \n",
        "    Required because EgoVLP/TimeSformer looks for this file statically in ./pretrained/\n",
        "    This function handles automatic download with progress tracking.\n",
        "    \n",
        "    Raises:\n",
        "        RuntimeError: If backbone download fails or connection is unavailable\n",
        "    \"\"\"\n",
        "    backbone_dir = \"pretrained\"\n",
        "    filename = \"jx_vit_base_p16_224-80ecf9dd.pth\"\n",
        "    file_path = os.path.join(backbone_dir, filename)\n",
        "    url = \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_224-80ecf9dd.pth\"\n",
        "\n",
        "    if not os.path.exists(backbone_dir):\n",
        "        os.makedirs(backbone_dir)\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"Backbone '{filename}' missing. Download in progress...\")\n",
        "        try:\n",
        "            # Use urllib with simple text progress bar\n",
        "            with tqdm(unit='B', unit_scale=True, miniters=1, desc=filename) as t:\n",
        "                def reporthook(blocknum, blocksize, totalsize):\n",
        "                    t.total = totalsize\n",
        "                    t.update(blocknum * blocksize - t.n)\n",
        "                urllib.request.urlretrieve(url, file_path, reporthook=reporthook)\n",
        "            print(\"âœ… Backbone download completed.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Backbone download error: {e}\")\n",
        "            raise RuntimeError(\"Unable to download base ImageNet weights. Check your connection.\")\n",
        "    else:\n",
        "        print(f\"âœ… ImageNet backbone found: {file_path}\")\n",
        "\n",
        "def setup_environment():\n",
        "    \"\"\"\n",
        "    Initializes the EgoVLP environment.\n",
        "    \n",
        "    Sets up:\n",
        "    - Python path to include EgoVLP repository\n",
        "    - Temporary directory for video processing\n",
        "    - GPU optimization flags\n",
        "    - Ensures backbone weights are downloaded\n",
        "    \"\"\"\n",
        "    if REPO_PATH not in sys.path:\n",
        "        sys.path.append(REPO_PATH)\n",
        "    if not os.path.exists(TEMP_WORK_DIR):\n",
        "        os.makedirs(TEMP_WORK_DIR)\n",
        "\n",
        "    # GPU optimization\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # Ensure base weights are present BEFORE importing or loading the model\n",
        "    ensure_pretrained_backbone()\n",
        "\n",
        "setup_environment()\n",
        "\n",
        "try:\n",
        "    from model.model import FrozenInTime\n",
        "except ImportError as e:\n",
        "    # Dynamic handling of missing dependencies\n",
        "    missing_module = e.name\n",
        "    print(f\"Critical import error: {e}\")\n",
        "\n",
        "    suggestion = \"\"\n",
        "    if missing_module == 'av':\n",
        "        suggestion = \"Run: !pip install av\"\n",
        "    elif missing_module == 'ffmpeg':\n",
        "        suggestion = \"Run: !pip install ffmpeg-python\"\n",
        "    elif missing_module == 'cv2':\n",
        "        suggestion = \"Run: !pip install opencv-python\"\n",
        "    else:\n",
        "        suggestion = f\"Try running: !pip install {missing_module}\"\n",
        "\n",
        "    raise RuntimeError(f\"Missing critical dependency. {suggestion}\")\n",
        "\n",
        "# --- Model Loading ---\n",
        "def get_egovlp_model(checkpoint_path, device='cuda'):\n",
        "    \"\"\"\n",
        "    Initializes and loads the EgoVLP model.\n",
        "    \n",
        "    Creates a FrozenInTime model with:\n",
        "    - SpaceTimeTransformer video encoder (16 frames per second)\n",
        "    - BERT text encoder\n",
        "    - 256-dimensional projection layer\n",
        "    \n",
        "    Args:\n",
        "        checkpoint_path: Path to the pre-trained EgoVLP checkpoint\n",
        "        device: Device to load model on ('cuda' or 'cpu')\n",
        "    \n",
        "    Returns:\n",
        "        model: Loaded EgoVLP model in evaluation mode\n",
        "    \n",
        "    Raises:\n",
        "        FileNotFoundError: If checkpoint path does not exist\n",
        "    \"\"\"\n",
        "    print(f\"Initializing model on {device}...\")\n",
        "    model = FrozenInTime(\n",
        "        video_params={\"model\": \"SpaceTimeTransformer\", \"pretrained\": True, \"num_frames\": 16},\n",
        "        text_params={\"model\": \"bert-base-uncased\", \"pretrained\": True},\n",
        "        projection_dim=256,\n",
        "        load_checkpoint=None\n",
        "    )\n",
        "\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)\n",
        "        state_dict = checkpoint['state_dict'] if 'state_dict' in checkpoint else checkpoint\n",
        "        new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
        "        model.load_state_dict(new_state_dict, strict=False)\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# --- Preprocessing ---\n",
        "def get_transform(input_size=224):\n",
        "    \"\"\"\n",
        "    Creates video preprocessing pipeline.\n",
        "    \n",
        "    Applies standard normalization for ImageNet pre-trained models:\n",
        "    - Rescale pixel values to [0, 1]\n",
        "    - Normalize using ImageNet statistics\n",
        "    - Center crop to (224, 224)\n",
        "    \n",
        "    Args:\n",
        "        input_size: Target spatial dimension (default: 224)\n",
        "    \n",
        "    Returns:\n",
        "        Compose: PyTorch transforms pipeline\n",
        "    \"\"\"\n",
        "    mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "    return Compose([\n",
        "        Lambda(lambda x: x / 255.0),\n",
        "        NormalizeVideo(mean, std),\n",
        "        CenterCropVideo(input_size),\n",
        "    ])\n",
        "\n",
        "# --- Feature Extraction ---\n",
        "def extract_features_per_second(model, video_path, device, batch_size_inference=16):\n",
        "    \"\"\"\n",
        "    Extracts EgoVLP features second-by-second from a video.\n",
        "    \n",
        "    For each second of the video:\n",
        "    1. Load video and compute FPS\n",
        "    2. Sample 16 frames evenly spaced within the second\n",
        "    3. Apply preprocessing (normalization, center crop)\n",
        "    4. Pass through EgoVLP model to get 256-dim features\n",
        "    5. Batch process for efficiency\n",
        "    \n",
        "    Args:\n",
        "        model: Pre-loaded EgoVLP model\n",
        "        video_path: Path to video file\n",
        "        device: Device to process on ('cuda' or 'cpu')\n",
        "        batch_size_inference: Number of seconds to process in parallel (default: 16)\n",
        "    \n",
        "    Returns:\n",
        "        np.ndarray: Features array of shape (num_seconds, 256)\n",
        "                   Returns None if video is too short (< 1 second)\n",
        "    \"\"\"\n",
        "    # Load video\n",
        "    vr = VideoReader(video_path, ctx=cpu(0))\n",
        "    fps = vr.get_avg_fps()\n",
        "    total_frames = len(vr)\n",
        "    duration_sec = int(total_frames / fps)\n",
        "\n",
        "    if duration_sec == 0:\n",
        "        return None  # Video too short (< 1 second)\n",
        "\n",
        "    transform = get_transform()\n",
        "\n",
        "    features_list = []\n",
        "    batch_buffer = []\n",
        "\n",
        "    # print(f\"  -> Video duration: {duration_sec}s. FPS: {fps:.2f}. Batch: {batch_size_inference}\")\n",
        "\n",
        "    for sec in range(duration_sec):\n",
        "        # Define the temporal interval of the current second\n",
        "        start_frame = int(sec * fps)\n",
        "        end_frame = int((sec + 1) * fps)\n",
        "\n",
        "        # Avoid index out of bounds\n",
        "        end_frame = min(end_frame, total_frames)\n",
        "\n",
        "        # Sample 16 frames evenly spaced in this second\n",
        "        if end_frame - start_frame < 16:\n",
        "             frame_indices = np.linspace(start_frame, end_frame - 1, 16, dtype=int)\n",
        "        else:\n",
        "             frame_indices = np.linspace(start_frame, end_frame - 1, 16, dtype=int)\n",
        "\n",
        "        raw_frames = vr.get_batch(frame_indices)  # (T, H, W, C)\n",
        "\n",
        "        if isinstance(raw_frames, torch.Tensor):\n",
        "            frames_tensor = raw_frames\n",
        "        else:\n",
        "            frames_tensor = torch.tensor(raw_frames.asnumpy())\n",
        "\n",
        "        # Standard PyTorch Video: (T, H, W, C) -> (C, T, H, W)\n",
        "        frames_tensor = frames_tensor.permute(3, 0, 1, 2).float()\n",
        "\n",
        "        # Normalization: input (C, T, H, W) -> output (C, T, H, W)\n",
        "        transformed_frames = transform(frames_tensor)\n",
        "\n",
        "        # EgoVLP requires: (Time, Channels, Height, Width) for single element\n",
        "        # Permute from (C, T, H, W) to (T, C, H, W)\n",
        "        transformed_frames = transformed_frames.permute(1, 0, 2, 3)\n",
        "\n",
        "        batch_buffer.append(transformed_frames)\n",
        "\n",
        "        # If buffer is full, perform inference\n",
        "        if len(batch_buffer) == batch_size_inference:\n",
        "            input_tensor = torch.stack(batch_buffer).to(device)\n",
        "            with torch.no_grad():\n",
        "                feat_batch = model({'video': input_tensor}, video_only=True)\n",
        "                features_list.append(feat_batch.cpu().numpy())\n",
        "            batch_buffer = []\n",
        "\n",
        "    # Process any remaining items in buffer\n",
        "    if len(batch_buffer) > 0:\n",
        "        input_tensor = torch.stack(batch_buffer).to(device)\n",
        "        with torch.no_grad():\n",
        "            feat_batch = model({'video': input_tensor}, video_only=True)\n",
        "            features_list.append(feat_batch.cpu().numpy())\n",
        "\n",
        "    # Concatenate all batches: output (Total_Seconds, 256)\n",
        "    if features_list:\n",
        "        return np.concatenate(features_list, axis=0)\n",
        "    return np.array([])\n",
        "\n",
        "def process_all_videos():\n",
        "    \"\"\"\n",
        "    Main processing function: extracts EgoVLP features for all videos.\n",
        "    \n",
        "    Workflow:\n",
        "    1. Initialize EgoVLP model on GPU/CPU\n",
        "    2. Discover all video files in VIDEO_DIR\n",
        "    3. For each video:\n",
        "       - Skip if features already exist\n",
        "       - Copy video to temporary local storage\n",
        "       - Extract features second-by-second\n",
        "       - Save features as compressed .npz file\n",
        "       - Clean up temporary files\n",
        "    4. Report success count\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    try:\n",
        "        model = get_egovlp_model(CHECKPOINT_PATH, device)\n",
        "    except Exception as e:\n",
        "        print(f\"Critical model error: {e}\")\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(FEATURES_DIR):\n",
        "        os.makedirs(FEATURES_DIR)\n",
        "\n",
        "    video_files = []\n",
        "    for ext in ['*.mp4', '*.MP4', '*.avi', '*.mov']:\n",
        "        video_files.extend(glob.glob(os.path.join(VIDEO_DIR, ext)))\n",
        "    video_files = sorted(list(set(video_files)))\n",
        "\n",
        "    print(f\"Found {len(video_files)} videos to process.\")\n",
        "\n",
        "    count_success = 0\n",
        "\n",
        "    # Progress bar with dynamic refresh\n",
        "    pbar = tqdm(video_files, desc=\"Feature Extraction\")\n",
        "\n",
        "    for drive_video_path in pbar:\n",
        "        video_name = os.path.basename(drive_video_path)\n",
        "        feature_filename = f\"{video_name}_1s_1s.npz\"\n",
        "        feature_save_path = os.path.join(FEATURES_DIR, feature_filename)\n",
        "\n",
        "        # --- Skip logic ---\n",
        "        if os.path.exists(feature_save_path):\n",
        "            pbar.set_postfix_str(f\"â© Skip: {video_name}\")\n",
        "            continue\n",
        "\n",
        "        local_temp_path = os.path.join(TEMP_WORK_DIR, video_name)\n",
        "\n",
        "        try:\n",
        "            pbar.set_postfix_str(f\"ðŸ”„ Processing: {video_name}\")\n",
        "            shutil.copy(drive_video_path, local_temp_path)\n",
        "\n",
        "            features = extract_features_per_second(model, local_temp_path, device)\n",
        "\n",
        "            if features is not None and len(features) > 0:\n",
        "                np.savez_compressed(feature_save_path, features=features)\n",
        "                count_success += 1\n",
        "            else:\n",
        "                print(f\"\\nNo features extracted for {video_name} (too short?)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError with {video_name}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "        finally:\n",
        "            if os.path.exists(local_temp_path):\n",
        "                os.remove(local_temp_path)\n",
        "\n",
        "    print(f\"\\nâœ… Completed. {count_success} new videos processed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_all_videos()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHIjw7L3cT2D"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0fcdbaccdfe245e88075a92bb4016813": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11b48ad8713743c38f5a9065e80c04c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b9f6ee0c733413ba440054f5729fc1b",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dea737b89b974ed89558614690f09643",
            "value": 570
          }
        },
        "1caccc5c629b4727bc50eb69203ee865": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4614ec70d5954d59baa7e63ac15901a9",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b2b6bf7f0694aa89c2fa0f23190c9d2",
            "value": 440449768
          }
        },
        "27b5f680ea6441d29a4feba3fd7581dd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2843b16408144187a16564cb0782b9c2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e14faabfa8440e487f07c02a0b6e8f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b22e3e9c0f646ed810c0adf9ed172db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f4c3682b6fc4dc3a3844eed5833082f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e14faabfa8440e487f07c02a0b6e8f0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5399c3887c9442afa28ccd42e100a6a9",
            "value": "model.safetensors:â€‡100%"
          }
        },
        "46117b76f40b48e28c117018832742ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4614ec70d5954d59baa7e63ac15901a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5399c3887c9442afa28ccd42e100a6a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b9f6ee0c733413ba440054f5729fc1b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b2b6bf7f0694aa89c2fa0f23190c9d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6eb8aa825295441aae803f362621c614": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a543ef167d994091b5a30ce573ad0e4a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e455de49065c479d9956123a7c2d15b8",
            "value": "â€‡570/570â€‡[00:00&lt;00:00,â€‡63.8kB/s]"
          }
        },
        "a543ef167d994091b5a30ce573ad0e4a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9de632ca6f34fcfa25223c5bce05f5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fcdbaccdfe245e88075a92bb4016813",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ef6588151fa54fe49ee6705f4e49dae7",
            "value": "config.json:â€‡100%"
          }
        },
        "c5e45f644d5c4eca8893fc4c3daf5bef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f4c3682b6fc4dc3a3844eed5833082f",
              "IPY_MODEL_1caccc5c629b4727bc50eb69203ee865",
              "IPY_MODEL_f75ff9cd770b444f9591ed9bafabb725"
            ],
            "layout": "IPY_MODEL_46117b76f40b48e28c117018832742ac"
          }
        },
        "c8f03be4fac6499f81d12c0428da867b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a9de632ca6f34fcfa25223c5bce05f5d",
              "IPY_MODEL_11b48ad8713743c38f5a9065e80c04c1",
              "IPY_MODEL_6eb8aa825295441aae803f362621c614"
            ],
            "layout": "IPY_MODEL_2843b16408144187a16564cb0782b9c2"
          }
        },
        "dea737b89b974ed89558614690f09643": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e455de49065c479d9956123a7c2d15b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef6588151fa54fe49ee6705f4e49dae7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f75ff9cd770b444f9591ed9bafabb725": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27b5f680ea6441d29a4feba3fd7581dd",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3b22e3e9c0f646ed810c0adf9ed172db",
            "value": "â€‡440M/440Mâ€‡[00:03&lt;00:00,â€‡247MB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
