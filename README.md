# AML 2025 - Progetto di Rilevazione Errori nelle Ricette

Questo progetto implementa un sistema di **rilevazione di errori in video di ricette egocentriche** utilizzando una combinazione di **Graph Neural Networks (GNNs)**, feature video e testuali estratte da modelli pre-addestrati.

## ðŸ“‹ Panoramica del Progetto

Il progetto affronta la seguente sfida: dato un video egocentrico di una ricetta, determinare se **l'esecutore ha commesso errori** durante la preparazione.

### Architettura Generale
1. **Feature Video** (EgoVLP + HiERO): Estrazione di embeddings video per ogni step della ricetta
2. **Feature Testuali** (EgoVLP): Estrazione di embeddings semantici dagli step testuali delle ricette
3. **Allineamento Multimodale** (Hungarian Algorithm + Temporal Cost): Matching tra video e testo
4. **Graph Neural Network (GraphSAGE)**: Classificazione binaria (errore/no errore) basata sul grafo della ricetta

---

## ðŸ“ Struttura del Google Drive (IMPORTANTE)

Per replicare correttamente il progetto, il professore deve creare la seguente struttura su Google Drive:

### Link Condiviso (fittizio per ora)
```
https://drive.google.com/drive/folders/1A2B3C4D5E6F7G8H9I0J1K2L3M4N5O6P7?usp=sharing
```
*Nota: Sostituire con il link reale quando il progetto Ã¨ pronto per la consegna*

### Struttura Gerarchica Completa

```
AML_Project/
â”œâ”€â”€ 3_EgoVLP/
â”‚   â”œâ”€â”€ checkpoints/
â”‚   â”‚   â””â”€â”€ egovlp.pth
â”‚   â”œâ”€â”€ EgoVLP-main/
â”‚   â”‚   â”œâ”€â”€ base/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ base_data_loader.py
â”‚   â”‚   â”‚   â”œâ”€â”€ base_dataset.py
â”‚   â”‚   â”‚   â””â”€â”€ ... (2 other files)
â”‚   â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â”‚   â”œâ”€â”€ eval/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ charades.json
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ egomcq.json
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ epic.json
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ... (2 other files)
â”‚   â”‚   â”‚   â”œâ”€â”€ ft/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ charades.json
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ epic.json
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ oscc.json
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ... (1 other files)
â”‚   â”‚   â”‚   â””â”€â”€ pt/
â”‚   â”‚   â”‚       â””â”€â”€ egoclip.json
â”‚   â”‚   â”œâ”€â”€ data_loader/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ CharadesEgo_dataset.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ConceptualCaptions_dataset.py
â”‚   â”‚   â”‚   â””â”€â”€ ... (9 other files)
â”‚   â”‚   â”œâ”€â”€ figures/
â”‚   â”‚   â”‚   â”œâ”€â”€ egomcq.jpg
â”‚   â”‚   â”‚   â””â”€â”€ egovlp_framework.jpg
â”‚   â”‚   â”œâ”€â”€ logger/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ logger.py
â”‚   â”‚   â”‚   â”œâ”€â”€ logger_config.json
â”‚   â”‚   â”‚   â””â”€â”€ ... (1 other files)
â”‚   â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ load_checkpoint.py
â”‚   â”‚   â”‚   â”œâ”€â”€ loss.py
â”‚   â”‚   â”‚   â””â”€â”€ ... (3 other files)
â”‚   â”‚   â”œâ”€â”€ run/
â”‚   â”‚   â”‚   â”œâ”€â”€ test_charades.py
â”‚   â”‚   â”‚   â”œâ”€â”€ test_epic.py
â”‚   â”‚   â”‚   â”œâ”€â”€ test_mq.py
â”‚   â”‚   â”‚   â””â”€â”€ ... (6 other files)
â”‚   â”‚   â”œâ”€â”€ trainer/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ trainer_charades.py
â”‚   â”‚   â”‚   â”œâ”€â”€ trainer_egoclip.py
â”‚   â”‚   â”‚   â””â”€â”€ ... (3 other files)
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ charades_meta.py
â”‚   â”‚   â”‚   â”œâ”€â”€ custom_transforms.py
â”‚   â”‚   â”‚   â””â”€â”€ ... (9 other files)
â”‚   â”‚   â”œâ”€â”€ environment.yml
â”‚   â”‚   â”œâ”€â”€ parse_config.py
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”œâ”€â”€ 10_16_360p_224.mp4_1s_1s.npy
â”‚   â”‚   â”œâ”€â”€ 10_16_360p_224.mp4_1s_1s.npz
â”‚   â”‚   â”œâ”€â”€ 10_18_360p_224.mp4_1s_1s.npy
â”‚   â”‚   â””â”€â”€ ... (765 other files)
â”‚   â”œâ”€â”€ pretrained/
â”‚   â”‚   â”œâ”€â”€ distilbert-base-uncased/
â”‚   â”‚   â”‚   â””â”€â”€ models--distilbert-base-uncased/
â”‚   â”‚   â”‚       â”œâ”€â”€ blobs/
â”‚   â”‚   â”‚       â”œâ”€â”€ refs/
â”‚   â”‚   â”‚       â””â”€â”€ snapshots/
â”‚   â”‚   â””â”€â”€ jx_vit_base_p16_224-80ecf9dd.pth
â”‚   â”œâ”€â”€ videos/
â”‚   â”‚   â”œâ”€â”€ 10_16_360p_224.mp4
â”‚   â”‚   â”œâ”€â”€ 10_18_360p_224.mp4
â”‚   â”‚   â”œâ”€â”€ 10_24_360p_224.mp4
â”‚   â”‚   â””â”€â”€ ... (381 other files)
â”‚   â””â”€â”€ EgoVLP_video_features.ipynb
â”œâ”€â”€ annotations-main/
â”‚   â”œâ”€â”€ annotation_csv/
â”‚   â”‚   â”œâ”€â”€ activity_idx_step_idx.csv
â”‚   â”‚   â”œâ”€â”€ activity_step_description.csv
â”‚   â”‚   â”œâ”€â”€ average_segment_length.csv
â”‚   â”‚   â””â”€â”€ ... (5 other files)
â”‚   â”œâ”€â”€ annotation_json/
â”‚   â”‚   â”œâ”€â”€ activity_idx_step_idx.json
â”‚   â”‚   â”œâ”€â”€ complete_step_annotations.json
â”‚   â”‚   â”œâ”€â”€ error_annotations (1).json
â”‚   â”‚   â””â”€â”€ ... (7 other files)
â”‚   â”œâ”€â”€ data_splits/
â”‚   â”‚   â”œâ”€â”€ environment_data_split_combined.json
â”‚   â”‚   â”œâ”€â”€ environment_data_split_normal.json
â”‚   â”‚   â”œâ”€â”€ person_data_split_combined.json
â”‚   â”‚   â””â”€â”€ ... (6 other files)
â”‚   â”œâ”€â”€ metadata/
â”‚   â”‚   â”œâ”€â”€ average_segment_length.csv
â”‚   â”‚   â””â”€â”€ video_information.csv
â”‚   â”œâ”€â”€ task_graphs/
â”‚   â”‚   â”œâ”€â”€ blenderbananapancakes.json
â”‚   â”‚   â”œâ”€â”€ breakfastburritos.json
â”‚   â”‚   â”œâ”€â”€ broccolistirfry.json
â”‚   â”‚   â””â”€â”€ ... (21 other files)
â”‚   â”œâ”€â”€ ANNOTATIONS.md
â”‚   â”œâ”€â”€ LICENSE
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ Extension/
â”‚   â”œâ”€â”€ step_1_HiERO/
â”‚   â”‚   â”œâ”€â”€ HiERO/
â”‚   â”‚   â”‚   â”œâ”€â”€ assets/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ hiero.png
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ teaser_animated.gif
â”‚   â”‚   â”‚   â”œâ”€â”€ checkpoints/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ hiero_egovlp.pth
â”‚   â”‚   â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ defaults.yaml
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ egovlp.yaml
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ lavila-l.yaml
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ... (1 other files)
â”‚   â”‚   â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ egoclip.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ egomcq.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ... (1 other files)
â”‚   â”‚   â”‚   â”œâ”€â”€ ego4d_goalstep/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ annotations/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ eval_grounding.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”‚   â”‚   â”œâ”€â”€ egoprocel/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ baseline_eval.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ configs.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ evaluate.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ... (3 other files)
â”‚   â”‚   â”‚   â”œâ”€â”€ features-extraction/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ extract.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ pipe.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ conv/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ext/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ tasks/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ hiero.py
â”‚   â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ dataloading.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ gradients.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ... (3 other files)
â”‚   â”‚   â”‚   â”œâ”€â”€ LICENSE
â”‚   â”‚   â”‚   â”œâ”€â”€ quickstart.ipynb
â”‚   â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”‚   â””â”€â”€ ... (3 other files)
â”‚   â”‚   â”œâ”€â”€ steps/
â”‚   â”‚   â”‚   â”œâ”€â”€ 10_16_steps.npz
â”‚   â”‚   â”‚   â”œâ”€â”€ 10_18_steps.npz
â”‚   â”‚   â”‚   â”œâ”€â”€ 10_24_steps.npz
â”‚   â”‚   â”‚   â””â”€â”€ ... (381 other files)
â”‚   â”‚   â”œâ”€â”€ HiERO.ipynb
â”‚   â”‚   â””â”€â”€ video_params_dump.csv
â”‚   â”œâ”€â”€ step_2_baseline/
â”‚   â”‚   â”œâ”€â”€ model_result/
â”‚   â”‚   â”‚   â”œâ”€â”€ best_model.pth
â”‚   â”‚   â”‚   â”œâ”€â”€ dataset_split_verification.png
â”‚   â”‚   â”‚   â”œâ”€â”€ master_split_ids.json
â”‚   â”‚   â”‚   â””â”€â”€ ... (1 other files)
â”‚   â”‚   â””â”€â”€ baseline.ipynb
â”‚   â”œâ”€â”€ step_3_task_graph/
â”‚   â”‚   â”œâ”€â”€ gnn_ready_data/
â”‚   â”‚   â”‚   â”œâ”€â”€ gnn_ready_10_16.pt
â”‚   â”‚   â”‚   â”œâ”€â”€ gnn_ready_10_18.pt
â”‚   â”‚   â”‚   â”œâ”€â”€ gnn_ready_10_24.pt
â”‚   â”‚   â”‚   â””â”€â”€ ... (381 other files)
â”‚   â”‚   â”œâ”€â”€ matched_features/
â”‚   â”‚   â”‚   â”œâ”€â”€ match_10_16.pt
â”‚   â”‚   â”‚   â”œâ”€â”€ match_10_18.pt
â”‚   â”‚   â”‚   â”œâ”€â”€ match_10_24.pt
â”‚   â”‚   â”‚   â””â”€â”€ ... (381 other files)
â”‚   â”‚   â”œâ”€â”€ pretrained/
â”‚   â”‚   â”‚   â””â”€â”€ jx_vit_base_p16_224-80ecf9dd.pth
â”‚   â”‚   â”œâ”€â”€ text_features_egovlp/
â”‚   â”‚   â”‚   â”œâ”€â”€ blenderbananapancakes.pt
â”‚   â”‚   â”‚   â”œâ”€â”€ breakfastburritos.pt
â”‚   â”‚   â”‚   â”œâ”€â”€ broccolistirfry.pt
â”‚   â”‚   â”‚   â””â”€â”€ ... (21 other files)
â”‚   â”‚   â””â”€â”€ Substep3.ipynb
â”‚   â””â”€â”€ step_4_gnn/
â”‚       â”œâ”€â”€ gnn_ready_data_groundtruth/
â”‚       â”‚   â”œâ”€â”€ gnn_ready_gt_10_16.pt
â”‚       â”‚   â”œâ”€â”€ gnn_ready_gt_10_18.pt
â”‚       â”‚   â”œâ”€â”€ gnn_ready_gt_10_24.pt
â”‚       â”‚   â””â”€â”€ ... (381 other files)
â”‚       â”œâ”€â”€ GroundTruth_GraphCreation.ipynb
â”‚       â”œâ”€â”€ Substep4_onGT.ipynb
â”‚       â””â”€â”€ Substep4V1.ipynb
â”œâ”€â”€ First_Part/
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”œâ”€â”€ omnivore.zip
â”‚   â”‚   â””â”€â”€ slowfast.zip
â”‚   â”œâ”€â”€ models_result_omnivore/
â”‚   â”‚   â”œâ”€â”€ lstm_recordings/
â”‚   â”‚   â”‚   â”œâ”€â”€ accuracy_plot.png
â”‚   â”‚   â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â”‚   â”‚   â”œâ”€â”€ final_lstm_report.csv
â”‚   â”‚   â”‚   â””â”€â”€ ... (2 other files)
â”‚   â”‚   â””â”€â”€ lstm_step/
â”‚   â”‚       â”œâ”€â”€ accuracy_plot.png
â”‚   â”‚       â”œâ”€â”€ confusion_matrix.png
â”‚   â”‚       â”œâ”€â”€ final_lstm_report.csv
â”‚   â”‚       â””â”€â”€ ... (2 other files)
â”‚   â”œâ”€â”€ models_result_slowfast/
â”‚   â”‚   â”œâ”€â”€ lstm_recordings/
â”‚   â”‚   â”‚   â”œâ”€â”€ accuracy_plot.png
â”‚   â”‚   â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â”‚   â”‚   â”œâ”€â”€ final_lstm_report.csv
â”‚   â”‚   â”‚   â””â”€â”€ ... (2 other files)
â”‚   â”‚   â””â”€â”€ lstm_step/
â”‚   â”‚       â”œâ”€â”€ accuracy_plot.png
â”‚   â”‚       â”œâ”€â”€ confusion_matrix.png
â”‚   â”‚       â”œâ”€â”€ final_lstm_report.csv
â”‚   â”‚       â””â”€â”€ ... (2 other files)
â”‚   â”œâ”€â”€ error_recognition_best.zip
â”‚   â”œâ”€â”€ Omnivore.ipynb
â”‚   â””â”€â”€ Slowfast.ipynb
â””â”€â”€ AML-2025_Mistake_Detection_Project.gdoc

```

---

## ðŸ” Descrizione Dettagliata dei Substep

### Substep 1: Estrazione Feature Video con HiERO e EgoVLP

**File Notebook**: `First_Part/EgoVLP_video_features.ipynb`, `First_Part/Omnivore.ipynb`, `First_Part/Slowfast.ipynb`, `Extension_Part/Substep_1/HiERO.ipynb`

**Relazione alla Consegna (AML-2025.pdf)**:
- Implementa l'**extraction di feature video** dalle ricette egocentriche
- Utilizza modelli pre-addestrati per estrarre embeddings semantici
- Applica **HiERO zero-shot** per segmentare automaticamente i video in step

**Cosa Viene Fatto**:
1. **Estrazione Feature EgoVLP**: Carica il modello pre-addestrato EgoVLP e estrae embeddings video di dimensione 256 con risoluzione temporale 1 frame/secondo
2. **Segmentazione HiERO**: Applica il modello HiERO zero-shot per identificare automaticamente i confini tra step nella ricetta
3. **Generazione Timestamp**: Estrae il timing di inizio e fine per ogni step rilevato
4. **Output**: File `.npz` contenenti embeddings e timestamp

**Output Generato**:
- `AML_Project/3_EgoVLP/features/{recording_id}_360p_224.mp4_1s_1s.npy` - Feature video raw
- `AML_Project/Extension/step_1_HiERO/steps/{recording_id}_steps.npz` - Step segmentati con embeddings

---

### Substep 2: Baseline Transformer e Split Consistente

**File Notebook**: `Extension_Part/Substep_2/baseline.ipynb`

**Relazione alla Consegna (AML-2025.pdf)**:
- Stabilisce una **baseline di performance** usando un modello Transformer semplice
- Fissa lo split train/val/test che verrÃ  mantenuto **identico** in tutti gli step successivi
- Consente il confronto tra la baseline e il modello GNN proposto

**Cosa Viene Fatto**:
1. **Caricamento Feature**: Legge i file `.npz` prodotti dal Substep 1
2. **Definizione Split**: Crea lo split train/val/test e lo salva in `master_split_ids.json`
3. **Modello Transformer**: Implementa un modello Transformer per classificazione binaria
4. **Addestramento e Valutazione**: Addestra il modello e riporta metriche di baseline
5. **Salvataggio Split**: Genera il file critico `master_split_ids.json` usato dagli step successivi

**Output Generato**:
- `AML_Project/Extension/step_2_baseline/model_result/master_split_ids.json`
- Metriche baseline (accuracy, F1, precision, recall)

---

### Substep 3: Feature Testuali e Costruzione Grafi Multimodali

**File Notebook**: `Extension_Part/Substep_3/Substep3.ipynb`

**Relazione alla Consegna (AML-2025.pdf)**:
- Implementa l'**integrazione multimodale**: combinazione di feature video e testuali
- Costruisce i **grafi canonici** delle ricette usando i dati di annotazione
- Applica **allineamento ottimo** tra i segment video predetti e gli step testuali della ricetta
- Prepara i dati per la **Graph Neural Network**

**Cosa Viene Fatto**:
1. **Estrazione Feature Testuali**:
   - Carica il modello EgoVLP
   - Estrae embeddings (dim: 256) per ogni step testuale della ricetta
   - Salva in `step_3_task_graph/text_features_egovlp/`

2. **Allineamento Video-Testo** (Hungarian Algorithm):
   - Calcola matrice di similaritÃ  coseno tra feature video e testuali
   - Aggiunge **penalitÃ  temporale** per favorire l'ordine cronologico corretto
   - Risolve il problema di assegnamento ottimo con algoritmo Hungarian
   - Output: matching tra video segments e recipe steps

3. **Costruzione Grafi**:
   - Legge il grafo canonico della ricetta da `task_graphs/{recipe_id}.json`
   - Allinea le feature video agli step del grafo tramite matching
   - Esegue late fusion: concatena feature video e testuali
   - Salva grafi in formato PyTorch Geometric

4. **Output**: Grafi pronti per la GNN in `step_3_task_graph/gnn_ready_data/`

**Output Generato**:
- `AML_Project/Extension/step_3_task_graph/text_features_egovlp/{recipe_id}.pt`
- `AML_Project/Extension/step_3_task_graph/matched_features/match_{recording_id}.pt`
- `AML_Project/Extension/step_3_task_graph/gnn_ready_data/gnn_ready_{recording_id}.pt`

---

### Substep 4: Classificazione GNN e Analisi Comparativa

**File Notebook**: `Extension_Part/Substep_4/Substep4V1.ipynb`, `Substep4_onGT.ipynb`, `GroundTruth_GraphCreation.ipynb`

**Relazione alla Consegna (AML-2025.pdf)**:
- Implementa la **Graph Neural Network (GraphSAGE)** per classificazione di errori
- Confronta le performance su **grafi predetti** vs **grafi Ground Truth ideali**
- Analizza l'impatto della qualitÃ  del grafo sulle performance finali

**Cosa Viene Fatto**:

#### 4.1 Substep4V1.ipynb (Classificazione su Grafi Predetti)
- Carica i grafi costruiti dal Substep 3 (`gnn_ready_data/`)
- Implementa architettura **GraphSAGE** con:
  - 2 layer convoluzionali
  - Batch normalization
  - Hybrid pooling (concatenazione Max + Mean)
  - Classificazione binaria con BCEWithLogitsLoss
- Addestra il modello con early stopping
- Valuta su test set e genera confusion matrix

#### 4.2 GroundTruth_GraphCreation.ipynb (Preparazione Grafi Ideali)
- Legge le annotazioni Ground Truth da `error_annotations.json`
- Estrae il timing corretto di ogni step da `complete_step_annotations.json`
- Estrae feature video usando il timing GT (non predetto)
- Costruisce grafi "ideali" in `step_4_gnn/gnn_ready_data_groundtruth/`
- Questi grafi hanno feature perfettamente allineate agli step reali

#### 4.3 Substep4_onGT.ipynb (Valutazione su Grafi Ground Truth)
- Carica lo stesso modello GNN addestrato su grafi predetti
- Valuta le performance sugli grafi Ground Truth
- Confronto critico:
  - **Se performance GT >> performance predetti**: Il problema Ã¨ nella qualitÃ  del grafo (Step 3)
  - **Se performance GT â‰ˆ performance predetti**: Il modello GNN Ã¨ maturo e robusto

**Output Generato**:
- `AML_Project/Extension/step_4_gnn/gnn_ready_data_groundtruth/gnn_ready_gt_{recording_id}.pt`
- `best_gnn_model.pth` - Modello GNN addestrato
- Metriche finali: Accuracy, F1-Score, Recall, Specificity, Confusion Matrix

---

## ðŸš€ Come Replicare il Progetto

### Prerequisiti
- Google Drive con almeno 50GB di spazio
- Google Colab con accesso a GPU
- Repository EgoVLP (link nel progetto)

### Step-by-Step

1. **Creare la struttura Google Drive** seguendo l'albero di cartelle descritto sopra

2. **Caricare i dati**:
   - Annotazioni JSON in `annotations-main/annotation_json/`
   - Grafi delle ricette in `annotations-main/task_graphs/`
   - Checkpoint EgoVLP in `3_EgoVLP/checkpoints/`

3. **Eseguire i Notebook in sequenza**:
   ```
   Substep 1 (HiERO.ipynb)
        â†“
   Substep 2 (baseline.ipynb) â†’ genera master_split_ids.json
        â†“
   Substep 3 (Substep3.ipynb) â†’ genera grafi predetti
        â†“
   Substep 4 (Substep4V1.ipynb) â†’ addestra GNN
   
   + Parallelo:
   Substep 4 (GroundTruth_GraphCreation.ipynb) â†’ grafi ideali
        â†“
   Substep 4 (Substep4_onGT.ipynb) â†’ valuta su GT
   ```

---